[
  {
    "indicator": "The amount of work in the system",
    "description": "",
    "metrics": [
      {
        "metric_name": "Total volume of work in progress - e.g. # of work items or total number of story points",
        "value": 107.0,
        "timeseries": [
          53.0,
          48.0,
          60.0,
          57.0,
          32.0,
          43.0,
          54.0,
          51.0,
          87.0,
          77.0,
          101.0,
          107.0
        ],
        "unit": "count",
        "y_axis_label": "Count"
      },
      {
        "metric_name": "Age of unfinished work (average age of work in progress)",
        "value": 30,
        "timeseries": [
          22.0,
          21.8,
          24.3,
          22.7,
          27.5,
          30,
          25.2,
          20.8,
          29.5,
          27.3,
          28.1,
          30
        ],
        "unit": "days",
        "y_axis_label": "Days"
      },
      {
        "metric_name": "# of sprints where departure is less than 40% of arrival",
        "value": 89.0,
        "timeseries": [
          88.6,
          79.5,
          88.7,
          93.9,
          65.8,
          74.4,
          100,
          94.8,
          96.9,
          99.7,
          95.0,
          89.0
        ],
        "unit": "%",
        "y_axis_label": "%"
      },
      {
        "metric_name": "WIP growth sprint-to-sprint",
        "value": 59.0,
        "timeseries": [
          70.0,
          66.0,
          65.0,
          63.0,
          64.0,
          71.0,
          38.0,
          64.0,
          69.0,
          64.0,
          62.0,
          59.0
        ],
        "unit": "count",
        "y_axis_label": "Count"
      }
    ],
    "data_source": "Jira",
    "children": [
      {
        "indicator": "Estimation effectiveness",
        "description": "",
        "metrics": [
          {
            "metric_name": "% of work items with no estimates",
            "value": 100,
            "timeseries": [
              71.5,
              77.5,
              56.7,
              67.9,
              71.9,
              79.2,
              91.8,
              85.3,
              90.3,
              100.0,
              100,
              100
            ],
            "unit": "%",
            "y_axis_label": "%"
          },
          {
            "metric_name": "Standard deviation of how long work items of the same estimate take",
            "value": 44.0,
            "timeseries": [
              29.0,
              25.0,
              24.0,
              17.0,
              15.0,
              49.0,
              25.0,
              44.0,
              32.0,
              31.0,
              40.0,
              44.0
            ],
            "unit": "count",
            "y_axis_label": "Count"
          },
          {
            "metric_name": "% of work items where the initial estimate changes",
            "value": 25.9,
            "timeseries": [
              7.7,
              7.8,
              11.1,
              6.4,
              0,
              2.2,
              27.3,
              7.4,
              0.1,
              0,
              26.6,
              25.9
            ],
            "unit": "%",
            "y_axis_label": "%"
          },
          {
            "metric_name": "% of work items where the initial work item is split",
            "value": 0,
            "timeseries": [
              38.7,
              31.7,
              36.8,
              40.5,
              40.8,
              36.2,
              16.8,
              6.9,
              2.1,
              0,
              1.4,
              0
            ],
            "unit": "%",
            "y_axis_label": "%"
          }
        ],
        "data_source": "Jira",
        "children": [
          {
            "indicator": "Consistency of estimation practices",
            "description": "",
            "metrics": [
              {
                "metric_name": "Team self-assessment score of consistency in regard to what we estimate (complexity, effort, uncertainty, risk, etc.)",
                "value": 10,
                "timeseries": [
                  9.8,
                  9.8,
                  10,
                  9.2,
                  8.5,
                  8.0,
                  8.1,
                  8.1,
                  8.5,
                  9.0,
                  9.4,
                  10
                ],
                "unit": "score",
                "y_axis_label": "Score (0\u201310)"
              },
              {
                "metric_name": "Team self-assessment score whether there's a DoD and whether they're estimating to it",
                "value": 1.5,
                "timeseries": [
                  1.4,
                  1.4,
                  1.5,
                  1.6,
                  3.8,
                  1.9,
                  1.6,
                  3.7,
                  0.0,
                  0,
                  0.8,
                  1.5
                ],
                "unit": "score",
                "y_axis_label": "Score (0\u201310)"
              },
              {
                "metric_name": "Team self-assessment score of how often they believe the team relies on 'historical record' to estimate or forecast",
                "value": 5.9,
                "timeseries": [
                  6.2,
                  6.1,
                  6.1,
                  6.1,
                  5.3,
                  4.8,
                  3.9,
                  3.3,
                  4.2,
                  5.1,
                  6.0,
                  5.9
                ],
                "unit": "score",
                "y_axis_label": "Score (0\u201310)"
              },
              {
                "metric_name": "Team self-assessment score of how confident they belive the team considers the end-to-end flow when estimating",
                "value": 0,
                "timeseries": [
                  0.1,
                  0,
                  0.1,
                  1.0,
                  2.4,
                  0.9,
                  0.6,
                  1.4,
                  0.4,
                  0,
                  0.8,
                  0
                ],
                "unit": "score",
                "y_axis_label": "Score (0\u201310)"
              },
              {
                "metric_name": "Team self-assessment score of how often work is estimated by someone other than the people doing the work",
                "value": 3.8,
                "timeseries": [
                  6.1,
                  6.4,
                  6.4,
                  5.8,
                  8.3,
                  7.8,
                  7.2,
                  7.2,
                  9.1,
                  4.4,
                  4.7,
                  3.8
                ],
                "unit": "score",
                "y_axis_label": "Score (0\u201310)"
              },
              {
                "metric_name": "Distribution of estimates and how close it is to the upper end",
                "value": 60.0,
                "timeseries": [
                  18.0,
                  27.0,
                  27.0,
                  24.0,
                  25.0,
                  32.0,
                  51.0,
                  60.0,
                  56.0,
                  65.0,
                  60.0,
                  60.0
                ],
                "unit": "count",
                "y_axis_label": "Count"
              }
            ],
            "data_source": "Self-assessment",
            "children": []
          },
          {
            "indicator": "The effectiveness of the refinement process",
            "description": "We cannot estimate effectively if we don't know enough about the work to be done. You need an effective refinement process in order to develop this 'good enough' understanding of the work need to estimate reasonably accurately",
            "metrics": [
              {
                "metric_name": "% of work items where there were no Acceptance Criteria added when moved to 'In Progress'",
                "value": 9.1,
                "timeseries": [
                  6.1,
                  8.8,
                  11.5,
                  14.8,
                  15.1,
                  7.4,
                  11.4,
                  24.2,
                  22.6,
                  22.2,
                  6.1,
                  9.1
                ],
                "unit": "%",
                "y_axis_label": "%"
              },
              {
                "metric_name": "% of time work spends in various analysis/refinement states before 'In Progress'",
                "value": 93.7,
                "timeseries": [
                  66.5,
                  61.3,
                  53.8,
                  53.2,
                  62.9,
                  55.1,
                  60.2,
                  89.3,
                  82.4,
                  76.1,
                  81.4,
                  93.7
                ],
                "unit": "%",
                "y_axis_label": "%"
              },
              {
                "metric_name": "% of work items not linked to a higher-level work item (e.g. user stories not connected to features)",
                "value": 79.6,
                "timeseries": [
                  92.0,
                  86.9,
                  78.8,
                  74.2,
                  75.3,
                  73.3,
                  79.5,
                  87.7,
                  92.7,
                  86.7,
                  84.7,
                  79.6
                ],
                "unit": "%",
                "y_axis_label": "%"
              },
              {
                "metric_name": "% of work items with changes to acceptance criteria/sizing/splitting the work item etc. after work starts on them",
                "value": 41.0,
                "timeseries": [
                  63.0,
                  56.0,
                  56.7,
                  64.3,
                  42.0,
                  51.8,
                  53.9,
                  49.3,
                  56.3,
                  48.8,
                  44.8,
                  41.0
                ],
                "unit": "%",
                "y_axis_label": "%"
              },
              {
                "metric_name": "% of work items where the details that were captured at the moment the work item was created do not change",
                "value": 86.9,
                "timeseries": [
                  38.9,
                  32.6,
                  39.8,
                  36.9,
                  17.5,
                  31.7,
                  47.5,
                  83.3,
                  100,
                  94.2,
                  93.7,
                  86.9
                ],
                "unit": "%",
                "y_axis_label": "%"
              },
              {
                "metric_name": "Team self-assessment score of the effectiveness of refinement (a. the team rates how often they start work feeling that there is important information/details missing about the piece of work",
                "value": 5.2,
                "timeseries": [
                  6.9,
                  6.8,
                  7.5,
                  7.1,
                  4.0,
                  4.8,
                  5.0,
                  4.8,
                  4.1,
                  4.8,
                  4.5,
                  5.2
                ],
                "unit": "score",
                "y_axis_label": "Score (0\u201310)"
              },
              {
                "metric_name": "b. the team rates how often they stop working on something after encountering a major unknown)",
                "value": 6.1,
                "timeseries": [
                  4.3,
                  5.2,
                  5.8,
                  5.9,
                  4.7,
                  5.4,
                  6.5,
                  6.0,
                  5.7,
                  8.1,
                  5.3,
                  6.1
                ],
                "unit": "score",
                "y_axis_label": "Score (0\u201310)"
              }
            ],
            "data_source": "Jira; Self-assessment",
            "children": []
          },
          {
            "indicator": "The volume of 'dark work'",
            "description": "Dark work obviously chips away from the capacity we can dedicate to taking new work, but because it's not visualised we can't count in when planning for a new increment, resulting in us not being able to accurately estimate how much work we can take in",
            "metrics": [
              {
                "metric_name": "Team self-assessment score of how much work they estimate they do that's NOT visualised anywhere",
                "value": 10,
                "timeseries": [
                  8.2,
                  8.6,
                  8.0,
                  9.5,
                  10,
                  9.6,
                  10,
                  10,
                  10,
                  10,
                  10,
                  10
                ],
                "unit": "score",
                "y_axis_label": "Score (0\u201310)"
              }
            ],
            "data_source": "Self-assessment",
            "children": []
          },
          {
            "indicator": "The volume of 'carryover'",
            "description": "Half-finished work is very difficult to estimate, resulting in us not being able to predict how much work remains (and therefore how much capacity to allocate to it). Also, a history of half-finished work being carried over to a new sprint/quarter means we can't rely on our historical record to estimate how much work we can typically finish in a single timebox",
            "metrics": [
              {
                "metric_name": "% of carry-over to total scope per sprint (or quarter) - calculating the # of carried over times or the total number of points being carried over.  2. % of work items that are finished in the same sprint they were started in",
                "value": 97.5,
                "timeseries": [
                  40.2,
                  41.2,
                  31.5,
                  40.0,
                  40.0,
                  46.2,
                  47.4,
                  82.0,
                  98.4,
                  52.5,
                  100,
                  97.5
                ],
                "unit": "%",
                "y_axis_label": "%"
              },
              {
                "metric_name": "% of features that were finished in the same quarter they were started in",
                "value": 91.9,
                "timeseries": [
                  64.8,
                  67.7,
                  71.5,
                  67.6,
                  73.3,
                  61.7,
                  77.6,
                  82.1,
                  89.5,
                  91.1,
                  91.8,
                  91.9
                ],
                "unit": "%",
                "y_axis_label": "%"
              }
            ],
            "data_source": "Jira",
            "children": []
          },
          {
            "indicator": "Avg work item size",
            "description": "The larger the work item, the more difficult it is to estimate it. Indicators of whether the work item size is appropriate include how often we finish a piece of work in the same time box we started it, the actual amount of work we spend on it, and the variation / consistency in how we size work items (if we don't have a standard definition of what a 3 is, and given how people usually tend to underestimate work, it's likely that we underestimate the real amount of work needed to deliver a piece of work - esp. work items on the larger side)",
            "metrics": [
              {
                "metric_name": "% of work items that are finished in the same sprint they were started in",
                "value": 51.4,
                "timeseries": [
                  9.1,
                  14.2,
                  8.9,
                  4.5,
                  0.2,
                  6.4,
                  1.9,
                  8.7,
                  9.9,
                  44.7,
                  53.4,
                  51.4
                ],
                "unit": "%",
                "y_axis_label": "%"
              },
              {
                "metric_name": "% of features that were finished in the same quarter they were started in",
                "value": 70.3,
                "timeseries": [
                  29.2,
                  38.7,
                  36.3,
                  43.8,
                  40.3,
                  72.6,
                  72.9,
                  66.5,
                  75.9,
                  78.6,
                  74.4,
                  70.3
                ],
                "unit": "%",
                "y_axis_label": "%"
              },
              {
                "metric_name": "Total Touch Time",
                "value": 13.1,
                "timeseries": [
                  5.3,
                  7.2,
                  7.2,
                  9.2,
                  6.2,
                  8.5,
                  5.9,
                  4.5,
                  6.4,
                  5.8,
                  8.7,
                  13.1
                ],
                "unit": "days",
                "y_axis_label": "Days"
              },
              {
                "metric_name": "Standard deviation of how long work items of the same estimate take",
                "value": 11.0,
                "timeseries": [
                  41.0,
                  37.0,
                  54.0,
                  29.0,
                  24.0,
                  30.0,
                  26.0,
                  17.0,
                  27.0,
                  28.0,
                  20.0,
                  11.0
                ],
                "unit": "count",
                "y_axis_label": "Count"
              },
              {
                "metric_name": "what percentile is this team/ToT in, in terms of how long on average a piece of work takes",
                "value": 97.8,
                "timeseries": [
                  66.9,
                  61.7,
                  68.2,
                  74.4,
                  69.6,
                  96.0,
                  100,
                  89.4,
                  92.8,
                  100,
                  100,
                  97.8
                ],
                "unit": "%",
                "y_axis_label": "%"
              },
              {
                "metric_name": "avg number of sprints a piece of work is carried over before it's 'done'",
                "value": 3.3,
                "timeseries": [
                  2.9,
                  2.2,
                  2.4,
                  1.8,
                  1.7,
                  2.2,
                  1.6,
                  1.3,
                  0.6,
                  1.8,
                  2.6,
                  3.3
                ],
                "unit": "sprints",
                "y_axis_label": "Number of Sprints"
              }
            ],
            "data_source": "Jira",
            "children": []
          },
          {
            "indicator": "Volume of dependencies",
            "description": "Dependencies affect our ability to estimate, because they affect our assessment of complexity. A piece with non-trivial dependencies is difficult to estimate because a sizable portion of it will be determined by an outside entity. Even if the piece of work to be estimated has no dependencies, the fact that the team is dealing with a lot of inter- and intra-team dependencies means that (1) we can't really rely much on historical record to relatively estimate a piece of work OR determine our true capacity",
            "metrics": [
              {
                "metric_name": "Dependency density for the time period in question (within the team)",
                "value": 92.0,
                "timeseries": [
                  44.0,
                  38.0,
                  40.0,
                  60.0,
                  48.0,
                  48.0,
                  52.0,
                  75.0,
                  77.0,
                  91.0,
                  90.0,
                  92.0
                ],
                "unit": "count",
                "y_axis_label": "Count"
              },
              {
                "metric_name": "% of work items with dependencies on other/outside teams",
                "value": 46.9,
                "timeseries": [
                  69.5,
                  66.1,
                  64.7,
                  81.5,
                  69.8,
                  60.7,
                  67.3,
                  28.8,
                  37.2,
                  36.9,
                  37.5,
                  46.9
                ],
                "unit": "%",
                "y_axis_label": "%"
              },
              {
                "metric_name": "% total story points affected by outside dependencies to total story points of the time interval",
                "value": 99.9,
                "timeseries": [
                  42.9,
                  34.0,
                  36.0,
                  41.2,
                  79.0,
                  88.5,
                  100,
                  92.8,
                  93.2,
                  94.2,
                  98.3,
                  99.9
                ],
                "unit": "%",
                "y_axis_label": "%"
              },
              {
                "metric_name": "Team self-assessment score of how often they stop working on an item waiting for an external dependency to be resolved",
                "value": 7.3,
                "timeseries": [
                  8.5,
                  8.6,
                  8.8,
                  8.5,
                  10,
                  7.5,
                  6.6,
                  6.1,
                  5.8,
                  5.5,
                  6.3,
                  7.3
                ],
                "unit": "score",
                "y_axis_label": "Score (0\u201310)"
              }
            ],
            "data_source": "Jira; Self-assessment",
            "children": []
          },
          {
            "indicator": "Team member allocation",
            "description": "If you have team members who are only partially allocated to a team (esp. if to more than 2 teams), then it becomes difficult to estimate the team's true capacity (too many things can go wrong in any of the teams of which this person is a member). It's difficult to forecast how much work you can do if you have a key team member who may or may not be able to work a certain number of hours/days this sprint or quarter",
            "metrics": [
              {
                "metric_name": "% of team members who are also being assigned work in other teams",
                "value": 100,
                "timeseries": [
                  66.0,
                  73.2,
                  82.7,
                  89.9,
                  61.8,
                  60.9,
                  55.6,
                  63.3,
                  100,
                  75.4,
                  67.3,
                  100
                ],
                "unit": "%",
                "y_axis_label": "%"
              },
              {
                "metric_name": "% of team members with a high variation in the total number of hours worked per timebox (sprint or quarter)",
                "value": 38.6,
                "timeseries": [
                  61.2,
                  65.1,
                  61.7,
                  53.2,
                  44.9,
                  41.0,
                  45.8,
                  41.9,
                  40.1,
                  32.4,
                  35.8,
                  38.6
                ],
                "unit": "%",
                "y_axis_label": "%"
              },
              {
                "metric_name": "changes in team measured through % of team members who have been contributing over n months",
                "value": 7.7,
                "timeseries": [
                  5.9,
                  5.2,
                  0,
                  0,
                  9.9,
                  0,
                  0,
                  8.1,
                  0,
                  24.9,
                  4.4,
                  7.7
                ],
                "unit": "%",
                "y_axis_label": "%"
              },
              {
                "metric_name": "Team self-assessment ~ team members to indicate whether or not they're 100% dedicated to the team",
                "value": 69.3,
                "timeseries": [
                  68.5,
                  75.5,
                  74.2,
                  64.6,
                  59.6,
                  53.4,
                  62.1,
                  59.9,
                  62.1,
                  58.0,
                  61.0,
                  69.3
                ],
                "unit": "%",
                "y_axis_label": "%"
              }
            ],
            "data_source": "Jira; Self-assessment",
            "children": []
          },
          {
            "indicator": "Reliability & consistency of yesterday's weather",
            "description": "If the data isn't consistent or there's high variance - one sprint our velocity is high, next one there's nothing, possibly due to large batches & constant interruption - then it's difficult to rely on 'yesterday's weather' to determine how much work to take in",
            "metrics": [
              {
                "metric_name": "Variance in throughput / velocity",
                "value": 37.0,
                "timeseries": [
                  53.0,
                  47.0,
                  55.0,
                  47.0,
                  51.0,
                  50.0,
                  36.0,
                  34.0,
                  37.0,
                  35.0,
                  42.0,
                  37.0
                ],
                "unit": "count",
                "y_axis_label": "Count"
              }
            ],
            "data_source": "Jira",
            "children": []
          }
        ]
      },
      {
        "indicator": "Ability to prioritise effectively",
        "description": "If we have a number of things that we consider important and must get done quickly, but we can\u2019t really decide which should be done first, then we might end up committing to / taking in too much new work",
        "metrics": [
          {
            "metric_name": "% of work items that are not linked to a higher-level work item or goal",
            "value": 0,
            "timeseries": [
              5.9,
              3.1,
              11.7,
              12.0,
              15.0,
              15.7,
              12.3,
              15.3,
              20.9,
              0,
              0,
              0
            ],
            "unit": "%",
            "y_axis_label": "%"
          },
          {
            "metric_name": "% of sprints with no sprint goals",
            "value": 19.4,
            "timeseries": [
              60.2,
              53.8,
              53.0,
              47.0,
              48.5,
              74.6,
              65.1,
              80.1,
              51.2,
              50.3,
              19.6,
              19.4
            ],
            "unit": "%",
            "y_axis_label": "%"
          },
          {
            "metric_name": "% of movement in the timebox (work coming out)",
            "value": 58.6,
            "timeseries": [
              64.7,
              55.1,
              64.2,
              56.7,
              53.8,
              50.0,
              32.1,
              42.8,
              61.4,
              56.6,
              59.9,
              58.6
            ],
            "unit": "%",
            "y_axis_label": "%"
          },
          {
            "metric_name": "% of movement in the sprint (new work going in)",
            "value": 32.7,
            "timeseries": [
              5.7,
              7.2,
              3.1,
              0.1,
              6.3,
              14.8,
              22.8,
              16.1,
              25.9,
              30.7,
              31.4,
              32.7
            ],
            "unit": "%",
            "y_axis_label": "%"
          },
          {
            "metric_name": "% of user stories that were moved to rejected/closed without being completed",
            "value": 97.9,
            "timeseries": [
              91.5,
              100,
              100,
              100,
              93.9,
              100,
              97.6,
              100,
              100,
              100,
              97.9,
              97.9
            ],
            "unit": "%",
            "y_axis_label": "%"
          },
          {
            "metric_name": "% of work items that have been in refinement/analysis more than a month and still weren't added to a sprint",
            "value": 84.5,
            "timeseries": [
              61.1,
              68.8,
              66.1,
              57.9,
              60.4,
              67.9,
              68.3,
              75.8,
              78.9,
              85.4,
              75.7,
              84.5
            ],
            "unit": "%",
            "y_axis_label": "%"
          },
          {
            "metric_name": "Team self-assessment of how often priorities / focus areas change",
            "value": 26.0,
            "timeseries": [
              10.0,
              2.0,
              0,
              0,
              10.0,
              17.0,
              19.0,
              12.0,
              17.0,
              22.0,
              31.0,
              26.0
            ],
            "unit": "count",
            "y_axis_label": "Count"
          }
        ],
        "data_source": "Jira; Self-assessment",
        "children": []
      },
      {
        "indicator": " Upstream capacity availability",
        "description": "carryover  plays a role in increasing WIP in different ways - here though, we're particularly focused on one aspect: that because a lot of carryover happens somewhat down stream, we feel tempted to bring in *new work* to our system (because the upstream is idle). Due to factors like the size of work items, our pattern of starting work, and the level of specialisation, we might find that most of the carried over work wasn't pending / in backlog. To keep the upstream busy/utilised, new work is brought into the system. This is why, despite having a lot of carry over, teams still opt to bring in a lot of new work.",
        "metrics": [
          {
            "metric_name": "% of carry-over work that was in dev (first stage in workflow)",
            "value": 49.7,
            "timeseries": [
              30.4,
              33.7,
              28.2,
              28.4,
              32.0,
              33.1,
              27.0,
              33.1,
              64.6,
              50.1,
              47.6,
              49.7
            ],
            "unit": "%",
            "y_axis_label": "%"
          }
        ],
        "data_source": "Jira",
        "children": [
          {
            "indicator": "Patterns of starting work",
            "description": "One reason why there's a lot of downstream carryover is because we tend to start many work items at once, so a big batch of work arrives at downstream steps late in the timebox (an examplew would be testing being idle at the start and then close to end of the sprint are overwhelmed with work)",
            "metrics": [
              {
                "metric_name": "Pattern of work arrival at downstream steps (taking them one by one). For each step, calculate the Coefficient of Variation and plot the arrival pattern",
                "value": 9.0,
                "timeseries": [
                  8.0,
                  4.0,
                  1.0,
                  3.0,
                  11.0,
                  3.0,
                  29.0,
                  8.0,
                  27.0,
                  0,
                  6.0,
                  9.0
                ],
                "unit": "count",
                "y_axis_label": "Count"
              },
              {
                "metric_name": "How many new work items we start every sprint vs. total committed scope AND total delivered scope",
                "value": 20.0,
                "timeseries": [
                  47.0,
                  43.0,
                  40.0,
                  40.0,
                  44.0,
                  36.0,
                  38.0,
                  14.0,
                  28.0,
                  23.0,
                  17.0,
                  20.0
                ],
                "unit": "count",
                "y_axis_label": "Count"
              },
              {
                "metric_name": "Team self-assessment ask QA to describe the pattern of work arrival",
                "value": 2.0,
                "timeseries": [
                  62.0,
                  54.0,
                  63.0,
                  74.0,
                  22.0,
                  48.0,
                  57.0,
                  42.0,
                  11.0,
                  8.0,
                  9.0,
                  2.0
                ],
                "unit": "count",
                "y_axis_label": "Count"
              }
            ],
            "data_source": "Jira",
            "children": []
          },
          {
            "indicator": "Cross-functionality",
            "description": "Lack of cross functionality - unwillingness, inability, or both - result in a situation where, despite having a lot of carried over work in downstream steps, those in upstream pull in new work instead of collaborating with those downstream to already in-progress work to done",
            "metrics": [
              {
                "metric_name": "Team self-assessment score of how often do they collaborate across-functions to get things done if there's work piling up",
                "value": 5.8,
                "timeseries": [
                  2.2,
                  3.1,
                  2.8,
                  3.6,
                  4.0,
                  3.0,
                  2.1,
                  2.1,
                  2.4,
                  2.8,
                  5.4,
                  5.8
                ],
                "unit": "score",
                "y_axis_label": "Score (0\u201310)"
              }
            ],
            "data_source": "Self-assessment",
            "children": []
          }
        ]
      },
      {
        "indicator": "Dependencies and blockers",
        "description": "Having a large number of dependencies that take a long time would mean that the team would be faced with either be idle waiting for things to be resolved OR pull in new work while the dependencies/blockers are being worked through. So, in addition to directly impacting cycle time, blockers and dependencies indirectly increase cycle time by creating incentive for the team to pull in more and more work",
        "metrics": [
          {
            "metric_name": "Avg time a blocker takes to be resolved (from the time the blocker was raised to the time it was resolved/lifted)",
            "value": 0,
            "timeseries": [
              12.0,
              17.0,
              16.0,
              24.0,
              23.0,
              32.0,
              32.0,
              11.0,
              10.0,
              9.0,
              0,
              0
            ],
            "unit": "count",
            "y_axis_label": "Count"
          },
          {
            "metric_name": "# of blockers raised per sprint",
            "value": 120,
            "timeseries": [
              108.0,
              108.0,
              112.0,
              114.0,
              92.0,
              120,
              120.0,
              120,
              120,
              119.0,
              112.0,
              120
            ],
            "unit": "count",
            "y_axis_label": "Count"
          },
          {
            "metric_name": "How much time is lost to dependencies (by comparing the avg time each work item size takes with and without dependencies)",
            "value": 43.0,
            "timeseries": [
              92.0,
              93.0,
              85.0,
              82.0,
              77.0,
              67.0,
              61.0,
              55.0,
              47.0,
              52.0,
              48.0,
              43.0
            ],
            "unit": "count",
            "y_axis_label": "Count"
          },
          {
            "metric_name": "Team self-assessment score of how often do they feel they have to pull in new work because existing work is blocked",
            "value": 5.2,
            "timeseries": [
              7.7,
              7.1,
              8.0,
              8.9,
              9.3,
              5.4,
              4.7,
              6.9,
              8.9,
              8.1,
              4.7,
              5.2
            ],
            "unit": "score",
            "y_axis_label": "Score (0\u201310)"
          }
        ],
        "data_source": "Jira; Self-assessment",
        "children": []
      },
      {
        "indicator": "Size of work items (this is specifically in the context of work coming into a timebox)",
        "description": "Large work items - on any level - mean that it's difficult to gauge how much to bring in and how much we've been able to do in the past, leading to overcommitment. Large work items also increase the likelihood of some parts of the pipeline being busy while others being idle, resulting in us bringing in too much work.",
        "metrics": [
          {
            "metric_name": "what percentile is this team/ToT in, in terms of how long on average a piece of work takes",
            "value": 42.2,
            "timeseries": [
              27.6,
              36.7,
              25.5,
              34.4,
              37.1,
              29.7,
              38.3,
              39.9,
              47.1,
              51.6,
              36.1,
              42.2
            ],
            "unit": "%",
            "y_axis_label": "%"
          },
          {
            "metric_name": "avg number of sprints a piece of work is carried over before it's 'done'",
            "value": 7.7,
            "timeseries": [
              7.6,
              8,
              7.2,
              5.6,
              8.0,
              7.7,
              8,
              7.7,
              8,
              8,
              7.4,
              7.7
            ],
            "unit": "sprints",
            "y_axis_label": "Number of Sprints"
          }
        ],
        "data_source": "Jira",
        "children": [
          {
            "indicator": "Team policies",
            "description": "Teams that do not have a policy that guide how big a work item can be and what to do if a work item is too large typically end up with work items that are too large and don't have an incentive to try and break them down",
            "metrics": [
              {
                "metric_name": "Team self-assessment score of how strongly they agree with the sentence that they have a clear definition of what 'too big' means in the context of work items and that they have a consistent approach of responding when a work item is considered too big",
                "value": 8.6,
                "timeseries": [
                  3.2,
                  3.2,
                  4.1,
                  5.0,
                  4.5,
                  5.2,
                  5.9,
                  5.7,
                  8.7,
                  10,
                  9.5,
                  8.6
                ],
                "unit": "score",
                "y_axis_label": "Score (0\u201310)"
              }
            ],
            "data_source": "Self-assessment",
            "children": []
          },
          {
            "indicator": "Consistency of estimation practices",
            "description": "",
            "metrics": [
              {
                "metric_name": "Team self-assessment score of consistency in regard to what we estimate (complexity, effort, uncertainty, risk, etc.)",
                "value": 7.3,
                "timeseries": [
                  0.3,
                  1.0,
                  0.9,
                  0.7,
                  0.7,
                  1.4,
                  4.1,
                  5.0,
                  6.0,
                  7.0,
                  6.9,
                  7.3
                ],
                "unit": "score",
                "y_axis_label": "Score (0\u201310)"
              },
              {
                "metric_name": "Team self-assessment score whether there's a DoD and whether they're estimating to it",
                "value": 7.0,
                "timeseries": [
                  3.3,
                  4.0,
                  4.1,
                  3.3,
                  3.0,
                  0,
                  0.2,
                  0.4,
                  0.1,
                  3.0,
                  7.0,
                  7.0
                ],
                "unit": "score",
                "y_axis_label": "Score (0\u201310)"
              },
              {
                "metric_name": "Team self-assessment score of how often they believe the team relies on 'historical record' to estimate or forecast",
                "value": 8.5,
                "timeseries": [
                  10.0,
                  9.9,
                  10,
                  9.0,
                  8.7,
                  8.5,
                  8.8,
                  8.1,
                  9.0,
                  9.0,
                  9.3,
                  8.5
                ],
                "unit": "score",
                "y_axis_label": "Score (0\u201310)"
              },
              {
                "metric_name": "Team self-assessment score of how confident they belive the team considers the end-to-end flow when estimating",
                "value": 6.3,
                "timeseries": [
                  7.9,
                  8.1,
                  7.8,
                  8.2,
                  5.4,
                  7.4,
                  7.9,
                  7.5,
                  6.5,
                  6.8,
                  6.5,
                  6.3
                ],
                "unit": "score",
                "y_axis_label": "Score (0\u201310)"
              },
              {
                "metric_name": "Team self-assessment score of how often work is estimated by someone other than the people doing the work",
                "value": 8.3,
                "timeseries": [
                  6.9,
                  7.3,
                  7.1,
                  6.9,
                  7.9,
                  8.1,
                  8.2,
                  7.6,
                  6.7,
                  7.6,
                  8.3,
                  8.3
                ],
                "unit": "score",
                "y_axis_label": "Score (0\u201310)"
              },
              {
                "metric_name": "Team self-assessment score of how comfortable they feel splitting work items / how hard it is",
                "value": 6.5,
                "timeseries": [
                  7.0,
                  7.7,
                  8.4,
                  8.1,
                  8.5,
                  7.7,
                  8.4,
                  6.6,
                  7.0,
                  7.3,
                  6.9,
                  6.5
                ],
                "unit": "score",
                "y_axis_label": "Score (0\u201310)"
              },
              {
                "metric_name": "Distribution of estimates and how close it is to the upper end",
                "value": 84.0,
                "timeseries": [
                  66.0,
                  59.0,
                  53.0,
                  57.0,
                  63.0,
                  67.0,
                  63.0,
                  56.0,
                  57.0,
                  64.0,
                  86.0,
                  84.0
                ],
                "unit": "count",
                "y_axis_label": "Count"
              }
            ],
            "data_source": "Jira; Self-assessment",
            "children": []
          },
          {
            "indicator": "The effectiveness of the refinement process",
            "description": "We cannot estimate effectively if we don't know enough about the work to be done. You need an effective refinement process in order to develop this 'good enough' understanding of the work need to estimate reasonably accurately",
            "metrics": [
              {
                "metric_name": "% of work items where there were no Acceptance Criteria added when moved to 'In Progress'",
                "value": 100,
                "timeseries": [
                  79.2,
                  85.2,
                  78.4,
                  76.5,
                  84.4,
                  86.0,
                  95.8,
                  89.5,
                  93.8,
                  99.3,
                  94.8,
                  100
                ],
                "unit": "%",
                "y_axis_label": "%"
              },
              {
                "metric_name": "% of time work spends in various analysis/refinement states before 'In Progress'",
                "value": 0,
                "timeseries": [
                  52.7,
                  43.1,
                  38.4,
                  28.7,
                  19.1,
                  25.5,
                  0,
                  0,
                  9.1,
                  3.6,
                  2.6,
                  0
                ],
                "unit": "%",
                "y_axis_label": "%"
              },
              {
                "metric_name": "% of work items not linked to a higher-level work item (e.g. user stories not connected to features)",
                "value": 57.2,
                "timeseries": [
                  65.1,
                  71.4,
                  67.7,
                  67.4,
                  49.4,
                  54.4,
                  55.1,
                  51.5,
                  56.6,
                  61.5,
                  52.5,
                  57.2
                ],
                "unit": "%",
                "y_axis_label": "%"
              },
              {
                "metric_name": "% of work items with changes to acceptance criteria/sizing/splitting the work item etc. after work starts on them",
                "value": 0,
                "timeseries": [
                  6.1,
                  4.8,
                  2.2,
                  0,
                  0,
                  1.4,
                  0,
                  0,
                  0.4,
                  0,
                  6.6,
                  0
                ],
                "unit": "%",
                "y_axis_label": "%"
              },
              {
                "metric_name": "% of work items where the details that were captured at the moment the work item was created do not change",
                "value": 86.7,
                "timeseries": [
                  61.1,
                  68.8,
                  77.9,
                  80.9,
                  82.1,
                  92.8,
                  100,
                  98.0,
                  98.3,
                  96.9,
                  79.6,
                  86.7
                ],
                "unit": "%",
                "y_axis_label": "%"
              },
              {
                "metric_name": "Team self-assessment score of the effectiveness of refinement (a. the team rates how often they start work feeling that there is important information/details missing about the piece of work",
                "value": 4.7,
                "timeseries": [
                  2.1,
                  2.6,
                  3.0,
                  3.2,
                  2.5,
                  4.5,
                  5.9,
                  2.1,
                  2.3,
                  2.8,
                  4.4,
                  4.7
                ],
                "unit": "score",
                "y_axis_label": "Score (0\u201310)"
              },
              {
                "metric_name": "b. the team rates how often they stop working on something after encountering a major unknown)",
                "value": 2.7,
                "timeseries": [
                  3.0,
                  3.6,
                  3.0,
                  3.8,
                  4.4,
                  3.5,
                  4.4,
                  3.5,
                  3.4,
                  4.3,
                  1.8,
                  2.7
                ],
                "unit": "score",
                "y_axis_label": "Score (0\u201310)"
              }
            ],
            "data_source": "Jira; Self-assessment",
            "children": []
          }
        ]
      },
      {
        "indicator": "Volume of interruptions",
        "description": "Unplanned interrptions lead to accumulating WIP - when we do not re-prioritise to incorporate the new priorities. Interruption demand can happen due to multiple reasons  - mostly are related to how we manage scope and allocate buffer, or when we accept new work and when we say no. The key thing to remember is that we're mostly concerned with the type of interruptions that creates WIP - that is, interruptions that are not followed by reprioritisation",
        "metrics": [
          {
            "metric_name": "avg % of interruption to total scope at the end of the timebox",
            "value": 80.1,
            "timeseries": [
              96.2,
              96.9,
              100,
              100,
              100,
              97.1,
              100,
              59.0,
              80.7,
              89.5,
              84.9,
              80.1
            ],
            "unit": "%",
            "y_axis_label": "%"
          },
          {
            "metric_name": "% of delta between how many work items (or how many total story points) were added mid-timebox vs removed before end of timebox to the total closing scope (scope at end of timebox) 3. % of sprints where that happens",
            "value": 0,
            "timeseries": [
              51.2,
              51.6,
              49.2,
              39.3,
              34.4,
              26.7,
              23.3,
              12.5,
              15.6,
              0,
              6.2,
              0
            ],
            "unit": "%",
            "y_axis_label": "%"
          }
        ],
        "data_source": "Jira",
        "children": [
          {
            "indicator": "Volume of 'new request' or scope change interrputions",
            "description": "One type of unplanned interruptions is change requests of all types - this refers to new 'change work' being added mid-timebox (without stuff leaving the backlog, ending up with over-commitment)",
            "metrics": [
              {
                "metric_name": "% of 'change' interruptions to overall interruptions (change interruptions can be identified as stories added, as opposed to bugs or taks )",
                "value": 92.1,
                "timeseries": [
                  63.3,
                  63.7,
                  73.3,
                  75.4,
                  70.7,
                  89.7,
                  89.2,
                  96.2,
                  99.2,
                  97.4,
                  83.0,
                  92.1
                ],
                "unit": "%",
                "y_axis_label": "%"
              },
              {
                "metric_name": "Team self-assessment score of the % of production defects they actually add to their Jira board",
                "value": 9.3,
                "timeseries": [
                  9.3,
                  8.9,
                  8.3,
                  9.1,
                  8.9,
                  9.5,
                  9.5,
                  10,
                  10,
                  9.8,
                  9.8,
                  9.3
                ],
                "unit": "score",
                "y_axis_label": "Score (0\u201310)"
              }
            ],
            "data_source": "Jira; Self-assessment",
            "children": [
              {
                "indicator": "Stakeholder visibility on short, medium and long-term goals",
                "description": "When stakeholders aren't clear on what we want to achieve now, next and later, they will be more likely interrupt us with new ideas, requests etc. - the more they know about what we want to do and how our plans are aligned with their goals and plans, the less likely it is for them to interrupt us",
                "metrics": [
                  {
                    "metric_name": "Team self-assessment score of how confident we feel that our stakeholders understand our short, medium and long-term goals",
                    "value": 9.2,
                    "timeseries": [
                      9.2,
                      9.9,
                      7.2,
                      9.2,
                      8.3,
                      7.8,
                      5.6,
                      8.3,
                      9.1,
                      8.6,
                      8.6,
                      9.2
                    ],
                    "unit": "score",
                    "y_axis_label": "Score (0\u201310)"
                  },
                  {
                    "metric_name": "Team self-assessment score of how confident we feel that we understand our stakeholders plans and goals (short, medium and long-term)",
                    "value": 6.4,
                    "timeseries": [
                      9.2,
                      8.6,
                      8.6,
                      8.9,
                      9.8,
                      10,
                      6.5,
                      7.8,
                      6.9,
                      6.3,
                      7.1,
                      6.4
                    ],
                    "unit": "score",
                    "y_axis_label": "Score (0\u201310)"
                  }
                ],
                "data_source": "Self-assessment",
                "children": []
              },
              {
                "indicator": "Stakeholder visibility on progress",
                "description": "Another dimension is that, without having visibility on how well we're progressing to achieve agreed-upon goals and targets, stakeholders might be more likely to intervene, fearing that certain key things aren't being addressed",
                "metrics": [
                  {
                    "metric_name": "Team self-assessment score of how often we hold sprint reviews (or similarly regular showcasing events)",
                    "value": 2.7,
                    "timeseries": [
                      0.3,
                      0.2,
                      0.9,
                      0.4,
                      0.8,
                      0.5,
                      0,
                      5.5,
                      6.0,
                      1.1,
                      1.9,
                      2.7
                    ],
                    "unit": "score",
                    "y_axis_label": "Score (0\u201310)"
                  },
                  {
                    "metric_name": "Team self-assessment score of the frequency of sprint reviews (or similarly regular showcasing events)",
                    "value": 5.1,
                    "timeseries": [
                      7.3,
                      7.7,
                      7.7,
                      7.8,
                      8.1,
                      7.7,
                      7.7,
                      5.9,
                      7.0,
                      7.0,
                      4.4,
                      5.1
                    ],
                    "unit": "score",
                    "y_axis_label": "Score (0\u201310)"
                  },
                  {
                    "metric_name": "Team self-assessment score of how often our key stakeholders attend our sprint reviews  (or similarly regular showcasing events)",
                    "value": 3.3,
                    "timeseries": [
                      7.6,
                      7.0,
                      6.8,
                      7.2,
                      7.4,
                      7.8,
                      7.7,
                      8.2,
                      8.0,
                      7.0,
                      7.0,
                      3.3
                    ],
                    "unit": "score",
                    "y_axis_label": "Score (0\u201310)"
                  }
                ],
                "data_source": "Self-assessment",
                "children": []
              },
              {
                "indicator": "Ability to push back against mid-timebox changes",
                "description": "Another dimension is that, without having visibility on how well we're progressing to achieve agreed-upon goals and targets, stakeholders might be more likely to intervene, fearing that certain key things aren't being addressed",
                "metrics": [
                  {
                    "metric_name": "Team self-assessment score of how strongly they agree that there exists a policy/process to determine when new work should be admitted and what happens next",
                    "value": 9.0,
                    "timeseries": [
                      9.2,
                      9.7,
                      9.9,
                      8.8,
                      10,
                      9.2,
                      10,
                      10,
                      9.2,
                      9.5,
                      9.3,
                      9.0
                    ],
                    "unit": "score",
                    "y_axis_label": "Score (0\u201310)"
                  },
                  {
                    "metric_name": "Team self-assessment score of how well they believe they as a team have been able to push back against mid-timebox interruptions",
                    "value": 5.5,
                    "timeseries": [
                      6.1,
                      6.6,
                      6.5,
                      6.9,
                      6.4,
                      6.5,
                      7.1,
                      6.2,
                      4.7,
                      5.5,
                      5.3,
                      5.5
                    ],
                    "unit": "score",
                    "y_axis_label": "Score (0\u201310)"
                  }
                ],
                "data_source": "Self-assessment",
                "children": []
              },
              {
                "indicator": "\"Buffer\" capacity allocation",
                "description": "If no adequate capacity was reserved for known unknowns - given how often we get interrupted in the past - then this may result in increased WIP (interruptions coming in but no buffer, resulting in adding new work to existing scope). We need to see not only that we leave buffer, but that we leave 'enough' buffer",
                "metrics": [
                  {
                    "metric_name": "Difference between avg available capacity (buffer) at the beginning of a timebox and avg mid-timebox  interruption (all types of interruption)",
                    "value": 85.0,
                    "timeseries": [
                      93.0,
                      86.0,
                      82.0,
                      76.0,
                      52.0,
                      80.0,
                      87.0,
                      89.0,
                      86.0,
                      89.0,
                      88.0,
                      85.0
                    ],
                    "unit": "count",
                    "y_axis_label": "Count"
                  }
                ],
                "data_source": "Jira",
                "children": []
              },
              {
                "indicator": "\"new request\" interruption predictability",
                "description": "Highly variable/unpredictable interrpution demand (new requests) makes it difficult to reserve buffer for potential changes, which increases the likelihood of over-committing at the start of the timebox",
                "metrics": [
                  {
                    "metric_name": "Variance in the volume of mid-timebox added stories (volume measured as % of scope at the end of the sprint)",
                    "value": 60.7,
                    "timeseries": [
                      54.7,
                      64.5,
                      52.5,
                      56.2,
                      61.9,
                      68.4,
                      58.8,
                      60.8,
                      67.6,
                      57.7,
                      56.2,
                      60.7
                    ],
                    "unit": "%",
                    "y_axis_label": "%"
                  }
                ],
                "data_source": "Jira",
                "children": []
              },
              {
                "indicator": "Clarity of change requests",
                "description": "If the change request has little information, this may cause back-and-forth and stopping to clarify what needs to be done, causing delays - delay increases the likelihood of taking up more work and increasing our WIP",
                "metrics": [
                  {
                    "metric_name": "Avg % of change requests with no description, comments or acceptance criteria added to the timebox",
                    "value": 82.6,
                    "timeseries": [
                      70.3,
                      77.0,
                      75.4,
                      80.6,
                      80.5,
                      83.3,
                      90.1,
                      89.9,
                      74.5,
                      75.4,
                      73.1,
                      82.6
                    ],
                    "unit": "%",
                    "y_axis_label": "%"
                  },
                  {
                    "metric_name": "Avg % of change requests with blockers or dependencies",
                    "value": 99.2,
                    "timeseries": [
                      91.0,
                      93.5,
                      99.0,
                      100,
                      94.0,
                      100,
                      83.6,
                      85.7,
                      92.6,
                      100,
                      90.8,
                      99.2
                    ],
                    "unit": "%",
                    "y_axis_label": "%"
                  },
                  {
                    "metric_name": "Team self-assessment score of how often they receive requests (change requests) with not enough information about what exactly needs to be done",
                    "value": 3.5,
                    "timeseries": [
                      3.9,
                      3.3,
                      3.1,
                      3.7,
                      4.7,
                      3.8,
                      3.7,
                      3.2,
                      4.0,
                      3.2,
                      3.2,
                      3.5
                    ],
                    "unit": "score",
                    "y_axis_label": "Score (0\u201310)"
                  }
                ],
                "data_source": "Jira; Self-assessment",
                "children": []
              },
              {
                "indicator": "Effectiveness of front-door",
                "description": "Teams where it's common for work to 'not' go through the PO and instead immeidately land on a team member's backlog have a higher risk of taking in too much work. Not having a single front door to the team means that work can come and be accepted without assessing capacity and team's existing priorities",
                "metrics": [
                  {
                    "metric_name": "% of work items in the timebox that were moved to done where the PO was not present at all (didn't create the work item",
                    "value": 65.3,
                    "timeseries": [
                      82.7,
                      91.6,
                      97.5,
                      94.5,
                      86.0,
                      77.3,
                      38.2,
                      42.4,
                      49.9,
                      53.6,
                      57.9,
                      65.3
                    ],
                    "unit": "%",
                    "y_axis_label": "%"
                  },
                  {
                    "metric_name": "hasn't commented on it",
                    "value": 26.0,
                    "timeseries": [
                      66.0,
                      57.0,
                      53.0,
                      48.0,
                      25.0,
                      0,
                      3.0,
                      4.0,
                      2.0,
                      3.0,
                      1.0,
                      26.0
                    ],
                    "unit": "count",
                    "y_axis_label": "Count"
                  },
                  {
                    "metric_name": "haven't moved it from one stage to another, etc.)",
                    "value": 20.9,
                    "timeseries": [
                      20.2,
                      23.1,
                      21.2,
                      23.6,
                      22.4,
                      21.7,
                      11.3,
                      15.3,
                      17.4,
                      16.7,
                      21.8,
                      20.9
                    ],
                    "unit": "days",
                    "y_axis_label": "Days"
                  },
                  {
                    "metric_name": "Team self-assessment score of how often work comes straight to team members without going to the PO first",
                    "value": 6.4,
                    "timeseries": [
                      4.0,
                      4.1,
                      4.4,
                      3.9,
                      4.0,
                      4.2,
                      4.3,
                      2.9,
                      5.8,
                      5.6,
                      6.4,
                      6.4
                    ],
                    "unit": "score",
                    "y_axis_label": "Score (0\u201310)"
                  }
                ],
                "data_source": "Jira; Self-assessment",
                "children": []
              },
              {
                "indicator": "Effectiveness of timebox planning",
                "description": "Ineffective planning means that work that wasn't well thought out finds its way to the sprint. Because it wasn't well thought out, the likelihood of us changing our minds and deciding that the work shouldn't be done now, or that the work isn't ready, or that something else more important ought to be worked on instead etc is high",
                "metrics": [
                  {
                    "metric_name": "% of work items where there were no Acceptance Criteria added when moved to 'In Progress'",
                    "value": 86.4,
                    "timeseries": [
                      97.7,
                      93.0,
                      93.0,
                      100,
                      100,
                      100,
                      100,
                      100,
                      100,
                      100,
                      94.1,
                      86.4
                    ],
                    "unit": "%",
                    "y_axis_label": "%"
                  },
                  {
                    "metric_name": "% of work items where there were no estimates added when moved to 'In Progress'",
                    "value": 94.3,
                    "timeseries": [
                      70.5,
                      63.2,
                      66.0,
                      69.6,
                      84.4,
                      76.8,
                      86.0,
                      86.5,
                      84.5,
                      93.2,
                      87.6,
                      94.3
                    ],
                    "unit": "%",
                    "y_axis_label": "%"
                  },
                  {
                    "metric_name": "% of time work spends in various analysis/refinement states before 'In Progress'",
                    "value": 30.0,
                    "timeseries": [
                      47.4,
                      51.5,
                      55.2,
                      50.5,
                      39.2,
                      48.4,
                      43.0,
                      18.5,
                      19.0,
                      20.1,
                      22.8,
                      30.0
                    ],
                    "unit": "%",
                    "y_axis_label": "%"
                  },
                  {
                    "metric_name": "% of work items not linked to a higher-level work item (e.g. user stories not connected to features)",
                    "value": 91.5,
                    "timeseries": [
                      83.3,
                      90.0,
                      88.3,
                      98.0,
                      100,
                      94.3,
                      99.1,
                      97.6,
                      100,
                      98.1,
                      100,
                      91.5
                    ],
                    "unit": "%",
                    "y_axis_label": "%"
                  },
                  {
                    "metric_name": "% of work items with changes to acceptance criteria/sizing/splitting the work item etc. after work starts on them",
                    "value": 0,
                    "timeseries": [
                      3.1,
                      0,
                      0,
                      0,
                      0,
                      0,
                      0,
                      8.8,
                      9.9,
                      3.8,
                      6.4,
                      0
                    ],
                    "unit": "%",
                    "y_axis_label": "%"
                  },
                  {
                    "metric_name": "% of work items where the details that were captured at the moment the work item was created do not change",
                    "value": 68.1,
                    "timeseries": [
                      81.7,
                      91.5,
                      86.1,
                      94.7,
                      94.6,
                      75.4,
                      95.3,
                      61.4,
                      62.0,
                      66.8,
                      61.2,
                      68.1
                    ],
                    "unit": "%",
                    "y_axis_label": "%"
                  },
                  {
                    "metric_name": "% of sprints where there was no sprint goal",
                    "value": 0,
                    "timeseries": [
                      33.6,
                      31.8,
                      37.6,
                      31.5,
                      28.8,
                      36.4,
                      46.3,
                      44.5,
                      6.1,
                      0,
                      0,
                      0
                    ],
                    "unit": "%",
                    "y_axis_label": "%"
                  },
                  {
                    "metric_name": "Team self-assessment score of the effectiveness of refinement (a. the team rates how often they start work feeling that there is important information/details missing about the piece of work",
                    "value": 0,
                    "timeseries": [
                      6.0,
                      5.2,
                      4.7,
                      3.8,
                      4.0,
                      3.5,
                      2.7,
                      1.9,
                      0.9,
                      0.9,
                      0,
                      0
                    ],
                    "unit": "score",
                    "y_axis_label": "Score (0\u201310)"
                  },
                  {
                    "metric_name": "b. the team rates how often they stop working on something after encountering a major unknown)",
                    "value": 7.5,
                    "timeseries": [
                      8.7,
                      9.2,
                      10,
                      8.3,
                      9.7,
                      10,
                      9.4,
                      10,
                      10,
                      9.5,
                      7.9,
                      7.5
                    ],
                    "unit": "score",
                    "y_axis_label": "Score (0\u201310)"
                  }
                ],
                "data_source": "Jira; Self-assessment",
                "children": []
              },
              {
                "indicator": "Clarity of short-, medium-, and long-term goals",
                "description": "Lack of an anchoring 'big picture' makes setting sprint goals and determining what should be done now, next and later difficult. Lack of such a picture forces the team to think tactically only, which leads to frequent pivots as it's difficult to prioritise when the long-term vision is not well understood/articulated",
                "metrics": [
                  {
                    "metric_name": "Team self-assessment score of how well they believe they understand what the team is trying to achieve in the next 3, 6, and 12 months",
                    "value": 2.8,
                    "timeseries": [
                      0.9,
                      1.8,
                      2.0,
                      1.3,
                      1.0,
                      1.4,
                      0.7,
                      4.5,
                      4.1,
                      3.2,
                      2.9,
                      2.8
                    ],
                    "unit": "score",
                    "y_axis_label": "Score (0\u201310)"
                  }
                ],
                "data_source": "Self-assessment",
                "children": []
              }
            ]
          },
          {
            "indicator": "Volume of interruptions caused by BAU demand",
            "description": "This is the \u2018Run the Product\u2019 stuff \u2013 anything that\u2019s not directly related to offering new capability or features to our customers, and NOT continuous improvement effort (e.g. refactoring). Minor changes to the product, platform changes/config (when we own the platform that others are leveraging), requests to specific team members due to their expertise or the fact that they\u2019re still the person responsible for tool X or asset Y, etc. -- every team deals with this, obviously - and, if it's an ops team, this IS the work - but we're trying to capture how *anticipated* this is",
            "metrics": [
              {
                "metric_name": "% of 'BAU' interruptions to overall interruptions (BAU interruptions can be identified as tasks added, as opposed to bugs or stories )",
                "value": 36.7,
                "timeseries": [
                  67.9,
                  73.5,
                  80.6,
                  81.7,
                  73.4,
                  66.1,
                  58.7,
                  61.0,
                  49.9,
                  42.1,
                  42.7,
                  36.7
                ],
                "unit": "%",
                "y_axis_label": "%"
              },
              {
                "metric_name": "Team self-assessment score of the % of production defects they actually add to their Jira board",
                "value": 4.8,
                "timeseries": [
                  2.7,
                  2.0,
                  2.0,
                  1.4,
                  0.7,
                  0,
                  0.2,
                  0.9,
                  4.8,
                  4.3,
                  4.6,
                  4.8
                ],
                "unit": "score",
                "y_axis_label": "Score (0\u201310)"
              }
            ],
            "data_source": "Jira; Self-assessment",
            "children": [
              {
                "indicator": "BAU interruption predictability",
                "description": "Highly variable/unpredictable interrpution demand (BAU) makes it difficult to reserve buffer for potential requests, which increases the likelihood of over-committing at the start of the timebox",
                "metrics": [
                  {
                    "metric_name": "Variance in the volume of mid-timebox added BAU tasks (volume measured as % of scope at the end of the sprint)",
                    "value": 84.1,
                    "timeseries": [
                      69.3,
                      67.0,
                      68.9,
                      68.8,
                      58.1,
                      77.5,
                      60.9,
                      98.0,
                      99.5,
                      100,
                      92.5,
                      84.1
                    ],
                    "unit": "%",
                    "y_axis_label": "%"
                  }
                ],
                "data_source": "Jira",
                "children": []
              },
              {
                "indicator": "\"Buffer\" capacity allocation",
                "description": "If no adequate capacity was reserved for known unknowns - given how often we get interrupted in the past - then this may result in increased WIP (interruptions coming in but no buffer, resulting in adding new work to existing scope). We need to see not only that we leave buffer, but that we leave 'enough' buffer",
                "metrics": [
                  {
                    "metric_name": "Difference between avg available capacity (buffer) at the beginning of a timebox and avg mid-timebox  interruption (all types of interruption)",
                    "value": 66.0,
                    "timeseries": [
                      52.0,
                      44.0,
                      51.0,
                      61.0,
                      60.0,
                      49.0,
                      80.0,
                      100.0,
                      97.0,
                      77.0,
                      70.0,
                      66.0
                    ],
                    "unit": "count",
                    "y_axis_label": "Count"
                  }
                ],
                "data_source": "Jira",
                "children": []
              },
              {
                "indicator": "Clarity of BAU requests",
                "description": "If the BAU request has little information, this may cause back-and-forth and stopping to clarify what needs to be done, causing delays - delay increases the likelihood of taking up more work and increasing our WIP ",
                "metrics": [
                  {
                    "metric_name": "Avg % of BAU requests with no description, comments or acceptance criteria added to the timebox",
                    "value": 79.0,
                    "timeseries": [
                      34.9,
                      30.7,
                      29.8,
                      31.6,
                      39.4,
                      65.1,
                      82.7,
                      47.8,
                      56.3,
                      84.6,
                      75.2,
                      79.0
                    ],
                    "unit": "%",
                    "y_axis_label": "%"
                  },
                  {
                    "metric_name": "Avg % of BAU requests with blockers or dependencies",
                    "value": 32.9,
                    "timeseries": [
                      61.8,
                      55.9,
                      62.3,
                      66.2,
                      75.4,
                      71.3,
                      92.1,
                      75.0,
                      78.7,
                      44.8,
                      37.6,
                      32.9
                    ],
                    "unit": "%",
                    "y_axis_label": "%"
                  },
                  {
                    "metric_name": "Team self-assessment score of how often they receive requests (BAU requests) with not enough information about what exactly needs to be done",
                    "value": 3.7,
                    "timeseries": [
                      5.3,
                      4.5,
                      4.8,
                      4.0,
                      5.3,
                      4.4,
                      5.0,
                      3.6,
                      3.1,
                      3.8,
                      4.6,
                      3.7
                    ],
                    "unit": "score",
                    "y_axis_label": "Score (0\u201310)"
                  }
                ],
                "data_source": "Jira; Self-assessment",
                "children": []
              },
              {
                "indicator": "Effectiveness of BAU triage",
                "description": "Not every request we receive is critical enough be addressed immediately at the risk of disrupting the timebox. An effective triage process would ensure that we have systemic checks we perform to ascertain the criticality of the request we're receiving",
                "metrics": [
                  {
                    "metric_name": "Avg % of BAU requests that were added to the 'product' backlog vs. the 'sprint' backlog",
                    "value": 2.2,
                    "timeseries": [
                      26.5,
                      23.9,
                      11.6,
                      11.0,
                      0,
                      4.4,
                      0.3,
                      0,
                      0,
                      0,
                      0,
                      2.2
                    ],
                    "unit": "%",
                    "y_axis_label": "%"
                  },
                  {
                    "metric_name": "Team self-assessment score of how effective the team thinks their BAU triage process is (in ensuring they only work on the most critical stuff)",
                    "value": 3.6,
                    "timeseries": [
                      0.2,
                      0.2,
                      1.1,
                      3.4,
                      0,
                      1.2,
                      1.4,
                      4.4,
                      3.2,
                      2.8,
                      3.5,
                      3.6
                    ],
                    "unit": "score",
                    "y_axis_label": "Score (0\u201310)"
                  }
                ],
                "data_source": "Jira; Self-assessment",
                "children": []
              },
              {
                "indicator": "Average size of interrupting BAU requests",
                "description": "large BAU requests that come mid time-box could result in an increase in total WIP volume. Large requests increase carryover (from the BAU requests themselves and/or the work that they stopped, as well as the potential of stoppages increases as the work item size increases -- unknowns, risks etc increase with size)",
                "metrics": [
                  {
                    "metric_name": "% of BAU interruptions that are finished in the same sprint they were started in (+compare with other work)",
                    "value": 79.0,
                    "timeseries": [
                      51.6,
                      60.6,
                      70.1,
                      68.0,
                      76.3,
                      79.0,
                      75.3,
                      41.9,
                      65.5,
                      71.1,
                      79.3,
                      79.0
                    ],
                    "unit": "%",
                    "y_axis_label": "%"
                  },
                  {
                    "metric_name": "Total Touch Time for BAU interruptions (+compare with other work)",
                    "value": 11.1,
                    "timeseries": [
                      25.9,
                      23.2,
                      17.2,
                      21.5,
                      19.0,
                      17.7,
                      18.5,
                      20.8,
                      20.3,
                      12.5,
                      21.6,
                      11.1
                    ],
                    "unit": "days",
                    "y_axis_label": "Days"
                  },
                  {
                    "metric_name": "Standard deviation of how long work items of the same estimate take (+compare with other work)",
                    "value": 50.0,
                    "timeseries": [
                      59.0,
                      56.0,
                      61.0,
                      66.0,
                      72.0,
                      81.0,
                      82.0,
                      86.0,
                      90.0,
                      81.0,
                      50.0,
                      50.0
                    ],
                    "unit": "count",
                    "y_axis_label": "Count"
                  },
                  {
                    "metric_name": "what percentile is this team/ToT in, in terms of how long on average a piece of work takes (+compare with BAU interruptions avg for other teams AND other types of work for this team and others)",
                    "value": 46.2,
                    "timeseries": [
                      32.3,
                      34.9,
                      25.0,
                      22.7,
                      25.2,
                      18.4,
                      11.8,
                      41.7,
                      10.0,
                      0,
                      6.5,
                      46.2
                    ],
                    "unit": "%",
                    "y_axis_label": "%"
                  },
                  {
                    "metric_name": "avg number of sprints a piece of work is carried over before it's 'done' (+compare to other work)",
                    "value": 1.8,
                    "timeseries": [
                      2.0,
                      1.8,
                      1.5,
                      1.1,
                      2.8,
                      1.8,
                      1.2,
                      0,
                      1.4,
                      1.6,
                      1.4,
                      1.8
                    ],
                    "unit": "sprints",
                    "y_axis_label": "Number of Sprints"
                  }
                ],
                "data_source": "Jira",
                "children": []
              }
            ]
          },
          {
            "indicator": "Volume of interruptions caused by production defects",
            "description": "Much like BAU interruption demand, production defects/escaped defects can result in increased WIP (among other things). The operating theory is that interruptions coupled with insufficient buffer will almost certainly lead to increased WIP. In terms of leading indicators, we're looking at (1) how we manage prod defects by looking at their predictability, the availability of buffer, our ability to prioritise (similar to BAU) etc., (2) and what's causing escaped defects in the first place (product quality)",
            "metrics": [
              {
                "metric_name": "% of 'Production defect' interruptions to overall interruptions (prod defect interruptions can be identified with 'prod-defect' label added to a task)",
                "value": 73.0,
                "timeseries": [
                  39.1,
                  30.6,
                  29.6,
                  33.8,
                  51.1,
                  59.1,
                  57.9,
                  57.4,
                  65.7,
                  71.8,
                  70.3,
                  73.0
                ],
                "unit": "%",
                "y_axis_label": "%"
              },
              {
                "metric_name": "Team self-assessment score of the % of production defects they actually add to their Jira board",
                "value": 10,
                "timeseries": [
                  9.1,
                  9.3,
                  9.1,
                  9.8,
                  9.8,
                  10,
                  10,
                  10,
                  7.5,
                  7.2,
                  9.9,
                  10
                ],
                "unit": "score",
                "y_axis_label": "Score (0\u201310)"
              },
              {
                "metric_name": "% of release rollbacks",
                "value": 93.4,
                "timeseries": [
                  63.2,
                  65.6,
                  63.4,
                  70.1,
                  65.3,
                  74.4,
                  64.7,
                  71.2,
                  87.3,
                  89.8,
                  85.6,
                  93.4
                ],
                "unit": "%",
                "y_axis_label": "%"
              }
            ],
            "data_source": "Jira; Self-assessment",
            "children": [
              {
                "indicator": "Prod defect interruption predictability (Prod defects management)",
                "description": "Highly variable/unpredictable interrpution demand (BAU) makes it difficult to reserve buffer for potential requests, which increases the likelihood of over-committing at the start of the timebox",
                "metrics": [
                  {
                    "metric_name": "Variance in the volume of mid-timebox added prod defect tasks (volume measured as % of scope at the end of the sprint)",
                    "value": 87.7,
                    "timeseries": [
                      89.7,
                      92.3,
                      95.5,
                      100,
                      95.2,
                      100,
                      72.7,
                      100,
                      100,
                      100,
                      93.5,
                      87.7
                    ],
                    "unit": "%",
                    "y_axis_label": "%"
                  }
                ],
                "data_source": "Jira",
                "children": []
              },
              {
                "indicator": "\"Buffer\" capacity allocation (Prod defects management)",
                "description": "If no adequate capacity was reserved for known unknowns - given how often we get interrupted in the past - then this may result in increased WIP (interruptions coming in but no buffer, resulting in adding new work to existing scope). We need to see not only that we leave buffer, but that we leave 'enough' buffer",
                "metrics": [
                  {
                    "metric_name": "Difference between avg available capacity (buffer) at the beginning of a timebox and avg mid-timebox  interruption (all types of interruption)",
                    "value": 35.0,
                    "timeseries": [
                      20.0,
                      26.0,
                      21.0,
                      15.0,
                      14.0,
                      18.0,
                      13.0,
                      25.0,
                      25.0,
                      24.0,
                      31.0,
                      35.0
                    ],
                    "unit": "count",
                    "y_axis_label": "Count"
                  }
                ],
                "data_source": "Jira",
                "children": []
              },
              {
                "indicator": "Clarity of prod defect requests (Prod defects management)",
                "description": "If the Prod defect request has little information, this may cause back-and-forth and stopping to clarify what needs to be done, causing delays - delay increases the likelihood of taking up more work and increasing our WIP ",
                "metrics": [
                  {
                    "metric_name": "Avg % of prod defect requests with no description, comments or acceptance criteria added to the timebox",
                    "value": 55.7,
                    "timeseries": [
                      50.5,
                      43.6,
                      28.2,
                      48.6,
                      35.7,
                      40.6,
                      37.3,
                      30.3,
                      21.9,
                      58.5,
                      62.1,
                      55.7
                    ],
                    "unit": "%",
                    "y_axis_label": "%"
                  },
                  {
                    "metric_name": "Avg % of prod defect requests with blockers or dependencies",
                    "value": 8.7,
                    "timeseries": [
                      9.1,
                      3.7,
                      11.5,
                      18.1,
                      21.4,
                      24.7,
                      19.3,
                      24.6,
                      21.1,
                      0,
                      0.7,
                      8.7
                    ],
                    "unit": "%",
                    "y_axis_label": "%"
                  },
                  {
                    "metric_name": "Team self-assessment score of how often they receive requests (prod defect requests) with not enough information about what exactly needs to be done",
                    "value": 0,
                    "timeseries": [
                      6.7,
                      7.0,
                      4.8,
                      7.8,
                      6.3,
                      3.8,
                      3.0,
                      0,
                      0,
                      0,
                      0,
                      0
                    ],
                    "unit": "score",
                    "y_axis_label": "Score (0\u201310)"
                  }
                ],
                "data_source": "Jira; Self-assessment",
                "children": []
              },
              {
                "indicator": "Effectiveness of Prod defect triage (Prod defects management)",
                "description": "Not every request we receive is critical enough be addressed immediately at the risk of disrupting the timebox. An effective triage process would ensure that we have systemic checks we perform to ascertain the criticality of the request we're receiving",
                "metrics": [
                  {
                    "metric_name": "Avg % of prod defect requests that were added to the 'product' backlog vs. the 'sprint' backlog",
                    "value": 88.5,
                    "timeseries": [
                      56.1,
                      56.1,
                      53.2,
                      51.9,
                      42.4,
                      37.3,
                      34.6,
                      29.3,
                      55.7,
                      80.5,
                      87.4,
                      88.5
                    ],
                    "unit": "%",
                    "y_axis_label": "%"
                  },
                  {
                    "metric_name": "Team self-assessment score of how effective the team thinks their Prod defect triage process is (in ensuring they only work on the most critical stuff)",
                    "value": 4.6,
                    "timeseries": [
                      5.6,
                      4.7,
                      4.2,
                      4.4,
                      5.3,
                      5.2,
                      6.2,
                      4.5,
                      7.4,
                      5.2,
                      4.7,
                      4.6
                    ],
                    "unit": "score",
                    "y_axis_label": "Score (0\u201310)"
                  }
                ],
                "data_source": "Jira; Self-assessment",
                "children": []
              },
              {
                "indicator": "Average size of interrupting Prod defect requests (Prod defects management)",
                "description": "large BAU requests that come mid time-box could result in an increase in total WIP volume. Large requests increase carryover (from the BAU requests themselves and/or the work that they stopped, as well as the potential of stoppages increases as the work item size increases -- unknowns, risks etc increase with size)",
                "metrics": [
                  {
                    "metric_name": "% of Prod defect interruptions that are finished in the same sprint they were started in (+compare with other work)",
                    "value": 46.9,
                    "timeseries": [
                      24.4,
                      34.2,
                      43.3,
                      34.1,
                      44.0,
                      38.3,
                      47.2,
                      62.4,
                      43.6,
                      42.5,
                      38.7,
                      46.9
                    ],
                    "unit": "%",
                    "y_axis_label": "%"
                  },
                  {
                    "metric_name": "Total Touch Time for prod defect interruptions (+compare with other work)",
                    "value": 6.4,
                    "timeseries": [
                      18.2,
                      20.9,
                      23.0,
                      20.9,
                      19.1,
                      19.2,
                      19.8,
                      10.7,
                      10.1,
                      10.0,
                      8.8,
                      6.4
                    ],
                    "unit": "days",
                    "y_axis_label": "Days"
                  },
                  {
                    "metric_name": "Standard deviation of how long work items of the same estimate take (+compare with other work)",
                    "value": 15.0,
                    "timeseries": [
                      24.0,
                      34.0,
                      33.0,
                      36.0,
                      33.0,
                      24.0,
                      9.0,
                      0,
                      7.0,
                      0,
                      8.0,
                      15.0
                    ],
                    "unit": "count",
                    "y_axis_label": "Count"
                  },
                  {
                    "metric_name": "what percentile is this team/ToT in, in terms of how long on average a piece of work takes (+compare with Prod defect interruptions avg for other teams AND other types of work for this team and others)",
                    "value": 57.3,
                    "timeseries": [
                      57.0,
                      59.7,
                      56.8,
                      66.1,
                      75.1,
                      58.9,
                      54.1,
                      21.2,
                      45.2,
                      54.4,
                      64.0,
                      57.3
                    ],
                    "unit": "%",
                    "y_axis_label": "%"
                  },
                  {
                    "metric_name": "avg number of sprints a piece of work is carried over before it's 'done' (+compare to other work)",
                    "value": 7.9,
                    "timeseries": [
                      3.7,
                      4.1,
                      4.5,
                      2.7,
                      4.3,
                      3.1,
                      3.7,
                      4.2,
                      4.5,
                      7.5,
                      7.7,
                      7.9
                    ],
                    "unit": "sprints",
                    "y_axis_label": "Number of Sprints"
                  }
                ],
                "data_source": "Jira",
                "children": []
              },
              {
                "indicator": "Product quality",
                "description": "There are two main components of prod defect-driven WIP increase: (1) we generate a lot of escaped defects due to lacking product quality, AND (2) we're not good at managing the defects coming (e.g. triaging what should be done now/later). The previous 4 metrics were assessing the management part, and this one focuses on the 'root cause' of this problem, namely quality",
                "metrics": [
                  {
                    "metric_name": "# of escaped defects (+compare to other teams)",
                    "value": 120,
                    "timeseries": [
                      117.0,
                      115.0,
                      120,
                      112.0,
                      100.0,
                      97.0,
                      99.0,
                      98.0,
                      97.0,
                      113.0,
                      120,
                      120
                    ],
                    "unit": "count",
                    "y_axis_label": "Count"
                  },
                  {
                    "metric_name": "'Reopened' Rate (after moved to Done)",
                    "value": 1.2,
                    "timeseries": [
                      1.4,
                      2.0,
                      1.6,
                      1.8,
                      1.4,
                      0,
                      0,
                      0,
                      0.8,
                      0.8,
                      4.0,
                      1.2
                    ],
                    "unit": "score",
                    "y_axis_label": "Score (0\u201310)"
                  },
                  {
                    "metric_name": "change failure rate",
                    "value": 7.9,
                    "timeseries": [
                      9.6,
                      9.6,
                      8.8,
                      8.3,
                      8.0,
                      7.1,
                      6.7,
                      6.7,
                      7.4,
                      7.5,
                      7.6,
                      7.9
                    ],
                    "unit": "score",
                    "y_axis_label": "Score (0\u201310)"
                  }
                ],
                "data_source": "Jira; Deployment tool",
                "children": [
                  {
                    "indicator": "Comprehensiveness & consistent use of Definition of Done",
                    "description": "The DoD includes all the checks that need to take place, all the various parts of the delivery machine that need to be involved, all the configurations that need to be done here and there, etc. before we can consider something 'done'. A less-than-comprehensive DoD - or a DoD that isn't always followed - may result in consistent quality, leading to an increase in prod defects",
                    "metrics": [
                      {
                        "metric_name": "Variance in how long work items spend in each step of the workflow for each work item size",
                        "value": 97.0,
                        "timeseries": [
                          74.0,
                          69.0,
                          68.0,
                          77.0,
                          76.0,
                          82.0,
                          90.0,
                          90.0,
                          99.0,
                          92.0,
                          100,
                          97.0
                        ],
                        "unit": "count",
                        "y_axis_label": "Count"
                      },
                      {
                        "metric_name": "the % of work items that are rejected/returned to the team following higher-level integration testing or release readiness testing",
                        "value": 74.4,
                        "timeseries": [
                          98.1,
                          98.5,
                          100,
                          93.2,
                          93.5,
                          100,
                          100,
                          97.0,
                          100,
                          100,
                          91.4,
                          74.4
                        ],
                        "unit": "%",
                        "y_axis_label": "%"
                      },
                      {
                        "metric_name": "Team self-assessment score of how clear, consistent and well-understood the process to get a piece of work to 'Done' is",
                        "value": 1.9,
                        "timeseries": [
                          1.6,
                          1.2,
                          0,
                          0.5,
                          0.9,
                          0.6,
                          0,
                          1.9,
                          0.9,
                          2.3,
                          1.3,
                          1.9
                        ],
                        "unit": "score",
                        "y_axis_label": "Score (0\u201310)"
                      }
                    ],
                    "data_source": "Jira; Self-assessment",
                    "children": []
                  },
                  {
                    "indicator": "Comprehensiveness & consistent use of Release readiness checklist",
                    "description": "In situations where the DoD doesn't go all the way to production release, we also want to ensure that the release readiness checklist is 'good enough' to ensure that we don't release stuff that shouldn't have been in production in the first place, resulting in high prod defect demand",
                    "metrics": [
                      {
                        "metric_name": "Variance in how long work items spend after marked 'Done' and released to prod",
                        "value": 38.0,
                        "timeseries": [
                          55.0,
                          55.0,
                          63.0,
                          68.0,
                          56.0,
                          58.0,
                          49.0,
                          45.0,
                          44.0,
                          47.0,
                          38.0,
                          38.0
                        ],
                        "unit": "count",
                        "y_axis_label": "Count"
                      },
                      {
                        "metric_name": "Team self-assessment score of how clear, consistent and well-understood the process to get a piece of work to 'Done'",
                        "value": 10,
                        "timeseries": [
                          9.8,
                          9.8,
                          9.3,
                          9.4,
                          10,
                          10,
                          9.9,
                          9.5,
                          8.7,
                          8.9,
                          9.5,
                          10
                        ],
                        "unit": "score",
                        "y_axis_label": "Score (0\u201310)"
                      }
                    ],
                    "data_source": "Jira; Self-assessment",
                    "children": []
                  },
                  {
                    "indicator": "How quality is baked into how we build the product",
                    "description": "If quality isn't embedded in how the team build the product, then it's inevitable that there will be prod defect demand ",
                    "metrics": [
                      {
                        "metric_name": "% of 'first pass' work items",
                        "value": 28.1,
                        "timeseries": [
                          2.6,
                          9.6,
                          0,
                          20.4,
                          44.9,
                          49.1,
                          54.4,
                          85.7,
                          26.5,
                          32.6,
                          37.2,
                          28.1
                        ],
                        "unit": "%",
                        "y_axis_label": "%"
                      },
                      {
                        "metric_name": "Avg # of times a piece of work has to go back to dev before it clears QA",
                        "value": 30.0,
                        "timeseries": [
                          29.0,
                          35.0,
                          41.0,
                          33.0,
                          22.0,
                          11.0,
                          6.0,
                          11.0,
                          41.0,
                          16.0,
                          23.0,
                          30.0
                        ],
                        "unit": "count",
                        "y_axis_label": "Count"
                      },
                      {
                        "metric_name": "bug rate (avg how many bugs per work item)",
                        "value": 6.1,
                        "timeseries": [
                          6.7,
                          7.0,
                          6.9,
                          6.1,
                          6.5,
                          7.2,
                          3.6,
                          6.5,
                          5.9,
                          5.1,
                          5.5,
                          6.1
                        ],
                        "unit": "score",
                        "y_axis_label": "Score (0\u201310)"
                      },
                      {
                        "metric_name": "Variance in how long on avg testing the same size work item takes",
                        "value": 62.0,
                        "timeseries": [
                          83.0,
                          78.0,
                          74.0,
                          81.0,
                          81.0,
                          79.0,
                          56.0,
                          54.0,
                          62.0,
                          58.0,
                          59.0,
                          62.0
                        ],
                        "unit": "count",
                        "y_axis_label": "Count"
                      },
                      {
                        "metric_name": "test fail rate (from Zephyr)",
                        "value": 2.1,
                        "timeseries": [
                          2.2,
                          2.4,
                          1.7,
                          1.0,
                          1.3,
                          2.0,
                          2.6,
                          3.0,
                          3.3,
                          2.7,
                          2.0,
                          2.1
                        ],
                        "unit": "score",
                        "y_axis_label": "Score (0\u201310)"
                      },
                      {
                        "metric_name": "PR approval rate (% of PRs approved vs. those requesting change)",
                        "value": 5.9,
                        "timeseries": [
                          6.5,
                          5.7,
                          5.7,
                          5.4,
                          5.6,
                          6.1,
                          5.5,
                          5.1,
                          4.8,
                          5.9,
                          5.8,
                          5.9
                        ],
                        "unit": "score",
                        "y_axis_label": "Score (0\u201310)"
                      },
                      {
                        "metric_name": "PR re-work rate (% of PRs that require changes after initial review) 8. PR rejection rate (% of PRs closed without merging)",
                        "value": 2.5,
                        "timeseries": [
                          4.8,
                          5.0,
                          4.7,
                          5.5,
                          5.9,
                          5.3,
                          6.6,
                          4.7,
                          4.2,
                          3.7,
                          3.0,
                          2.5
                        ],
                        "unit": "score",
                        "y_axis_label": "Score (0\u201310)"
                      },
                      {
                        "metric_name": "Code churn",
                        "value": 75.0,
                        "timeseries": [
                          85.0,
                          78.0,
                          84.0,
                          100,
                          81.0,
                          89.0,
                          84.0,
                          75.0,
                          62.0,
                          100,
                          77.0,
                          75.0
                        ],
                        "unit": "count",
                        "y_axis_label": "Count"
                      }
                    ],
                    "data_source": "Jira; Zephyr; Bitbucket",
                    "children": [
                      {
                        "indicator": "Effectiveness of code reviews",
                        "description": "The extent and effectiveness of code reviews are leading indicators to quality - our hypothesis is that subpar code reviews will result in higher defects making their way to QA, increasing the overall likelihood of escaped defects (and also slowing down testing, increasing WIP in other ways)",
                        "metrics": [
                          {
                            "metric_name": "PR size (Avg. # of lines changed/files modified per PR)",
                            "value": 24.0,
                            "timeseries": [
                              60.0,
                              68.0,
                              63.0,
                              55.0,
                              77.0,
                              67.0,
                              100.0,
                              63.0,
                              51.0,
                              62.0,
                              16.0,
                              24.0
                            ],
                            "unit": "count",
                            "y_axis_label": "Count"
                          },
                          {
                            "metric_name": "Avg time spent on code reviews",
                            "value": 38.0,
                            "timeseries": [
                              73.0,
                              75.0,
                              75.0,
                              77.0,
                              54.0,
                              80.0,
                              70.0,
                              62.0,
                              62.0,
                              54.0,
                              64.0,
                              38.0
                            ],
                            "unit": "count",
                            "y_axis_label": "Count"
                          },
                          {
                            "metric_name": "Review depth (# of comments per PR)",
                            "value": 100.0,
                            "timeseries": [
                              78.0,
                              74.0,
                              77.0,
                              82.0,
                              91.0,
                              92.0,
                              86.0,
                              79.0,
                              120,
                              120,
                              108.0,
                              100.0
                            ],
                            "unit": "count",
                            "y_axis_label": "Count"
                          },
                          {
                            "metric_name": "Avg. time between PR creation and first review",
                            "value": 16.0,
                            "timeseries": [
                              14.0,
                              17.0,
                              23.0,
                              20.0,
                              26.0,
                              27.0,
                              23.0,
                              0,
                              2.0,
                              12.0,
                              14.0,
                              16.0
                            ],
                            "unit": "count",
                            "y_axis_label": "Count"
                          },
                          {
                            "metric_name": "Avg number of reviewers per PR",
                            "value": 41.0,
                            "timeseries": [
                              56.0,
                              56.0,
                              48.0,
                              56.0,
                              47.0,
                              40.0,
                              31.0,
                              63.0,
                              57.0,
                              41.0,
                              37.0,
                              41.0
                            ],
                            "unit": "count",
                            "y_axis_label": "Count"
                          },
                          {
                            "metric_name": "Time to merge (avg. time from PR creation to merge)",
                            "value": 94.0,
                            "timeseries": [
                              85.0,
                              77.0,
                              77.0,
                              75.0,
                              75.0,
                              68.0,
                              69.0,
                              68.0,
                              74.0,
                              84.0,
                              91.0,
                              94.0
                            ],
                            "unit": "count",
                            "y_axis_label": "Count"
                          },
                          {
                            "metric_name": "comment resolution time (Avg. ime to address review comments)",
                            "value": 24.0,
                            "timeseries": [
                              0.0,
                              4.0,
                              4.0,
                              0,
                              0,
                              5.0,
                              0,
                              0,
                              0,
                              26.0,
                              18.0,
                              24.0
                            ],
                            "unit": "count",
                            "y_axis_label": "Count"
                          }
                        ],
                        "data_source": "Bitbucket",
                        "children": []
                      },
                      {
                        "indicator": "Work item size",
                        "description": "Larger work items are riskier, potentially leading to more quality issues (which increases the likelihood of a defect escaping QA and making its way to production). We hypothesise that the number of bugs / risks etc. grows non-linearly as work item size grows. It is harder to embed quality in a larger work item than a smaller work item, and more likely for a bug to escape QA in a larger piece of work than a smaller one",
                        "metrics": [
                          {
                            "metric_name": "% of work items that clear Dev in the same sprint they were started in",
                            "value": 1.1,
                            "timeseries": [
                              70.9,
                              64.4,
                              73.6,
                              66.7,
                              66.2,
                              66.8,
                              57.7,
                              54.8,
                              32.3,
                              52.4,
                              24.5,
                              1.1
                            ],
                            "unit": "%",
                            "y_axis_label": "%"
                          },
                          {
                            "metric_name": "Total Dev Touch Time",
                            "value": 24.6,
                            "timeseries": [
                              19.1,
                              22.0,
                              21.2,
                              22.7,
                              24.9,
                              25.0,
                              27.2,
                              28.9,
                              30.0,
                              27.0,
                              27.2,
                              24.6
                            ],
                            "unit": "days",
                            "y_axis_label": "Days"
                          },
                          {
                            "metric_name": "Standard deviation of how long in Dev work items of the same estimate take",
                            "value": 41.0,
                            "timeseries": [
                              57.0,
                              55.0,
                              58.0,
                              52.0,
                              69.0,
                              78.0,
                              76.0,
                              86.0,
                              76.0,
                              66.0,
                              48.0,
                              41.0
                            ],
                            "unit": "count",
                            "y_axis_label": "Count"
                          },
                          {
                            "metric_name": "what percentile is this team/ToT in, in terms of how long on average a piece of work takes",
                            "value": 93.6,
                            "timeseries": [
                              88.8,
                              86.8,
                              76.9,
                              79.3,
                              81.6,
                              74.4,
                              64.6,
                              74.6,
                              68.7,
                              59.5,
                              54.0,
                              93.6
                            ],
                            "unit": "%",
                            "y_axis_label": "%"
                          },
                          {
                            "metric_name": "avg number of sprints a piece of work is carried over before it clear development",
                            "value": 8,
                            "timeseries": [
                              7.6,
                              6.8,
                              5.3,
                              7.5,
                              8,
                              8,
                              6.0,
                              6.4,
                              5.7,
                              6.3,
                              6.0,
                              8
                            ],
                            "unit": "sprints",
                            "y_axis_label": "Number of Sprints"
                          }
                        ],
                        "data_source": "Jira",
                        "children": []
                      },
                      {
                        "indicator": "Unit test coverage",
                        "description": "Lower test coverage increases the risk of latent bugs that might make their way to prod",
                        "metrics": [
                          {
                            "metric_name": "% of code covered by unit tests",
                            "value": 63.6,
                            "timeseries": [
                              75.8,
                              81.4,
                              77.8,
                              79.1,
                              74.2,
                              80.4,
                              56.7,
                              61.2,
                              67.2,
                              59.2,
                              71.7,
                              63.6
                            ],
                            "unit": "%",
                            "y_axis_label": "%"
                          },
                          {
                            "metric_name": "Test coverage change (change in test coverage % per PR)",
                            "value": 0,
                            "timeseries": [
                              64.0,
                              54.2,
                              62.2,
                              55.4,
                              45.5,
                              46.1,
                              42.8,
                              43.0,
                              11.2,
                              7.9,
                              2.5,
                              0
                            ],
                            "unit": "%",
                            "y_axis_label": "%"
                          }
                        ],
                        "data_source": "Bitbucket",
                        "children": []
                      },
                      {
                        "indicator": "Code complexity",
                        "description": "we hypothesise that  complexity, measured through cyclomatic complexity (CC), is a good leading indicator to quality. The more complex the code is, the higher the likelihood of breaking something while trying to integrate something new AND the likelihood of not fixing a problem as intended (also higher likelihood of QA missing where the problem is)",
                        "metrics": [
                          {
                            "metric_name": "Code Complexity (1-5 low, 6-10 moderate, 11-20 high, 21+ very high)",
                            "value": 4.4,
                            "timeseries": [
                              3.9,
                              4.1,
                              2.4,
                              4.2,
                              3.6,
                              1.6,
                              4.7,
                              4.1,
                              4.4,
                              3.8,
                              3.8,
                              4.4
                            ],
                            "unit": "score",
                            "y_axis_label": "Score (0\u201310)"
                          },
                          {
                            "metric_name": "Maintainability Index (<10 difficult to maintain",
                            "value": 90.9,
                            "timeseries": [
                              84.6,
                              89.3,
                              79.4,
                              81.9,
                              75.0,
                              100,
                              76.5,
                              81.2,
                              87.2,
                              91.7,
                              86.7,
                              90.9
                            ],
                            "unit": "index",
                            "y_axis_label": "Index"
                          },
                          {
                            "metric_name": "between 10-20 moderately maintainable",
                            "value": 0.3,
                            "timeseries": [
                              2.5,
                              2.8,
                              2.5,
                              2.0,
                              1.0,
                              2.4,
                              1.3,
                              1.7,
                              2.4,
                              1.7,
                              0,
                              0.3
                            ],
                            "unit": "score",
                            "y_axis_label": "Score (0\u201310)"
                          },
                          {
                            "metric_name": ">20 highly maintainable)",
                            "value": 27.0,
                            "timeseries": [
                              62.0,
                              52.0,
                              50.0,
                              54.0,
                              55.0,
                              63.0,
                              38.0,
                              61.0,
                              24.0,
                              29.0,
                              35.0,
                              27.0
                            ],
                            "unit": "count",
                            "y_axis_label": "Count"
                          }
                        ],
                        "data_source": "Bitbucket",
                        "children": []
                      },
                      {
                        "indicator": "Dependencies",
                        "description": "Looking both at code and architectural dependencies - the hypothesis is that dependencies not only slow down development and testing, but also that the more complex our dependencies are, the higher the likelihood of introducing bugs (higher risk of inadvertently breaking something). The higher the likelihood of introducing bugs, the higher the likelihood of bugs eventually making it to production",
                        "metrics": [
                          {
                            "metric_name": "Code dependency index",
                            "value": 100,
                            "timeseries": [
                              70.5,
                              74.7,
                              70.3,
                              80.2,
                              87.9,
                              90.5,
                              98.7,
                              90.2,
                              99.1,
                              92.1,
                              100,
                              100
                            ],
                            "unit": "index",
                            "y_axis_label": "Index"
                          },
                          {
                            "metric_name": "Architectural dependency index",
                            "value": 62.2,
                            "timeseries": [
                              59.7,
                              62.0,
                              55.3,
                              65.2,
                              61.7,
                              65.7,
                              57.4,
                              55.3,
                              59.3,
                              59.6,
                              58.8,
                              62.2
                            ],
                            "unit": "index",
                            "y_axis_label": "Index"
                          }
                        ],
                        "data_source": "Bitbucket",
                        "children": []
                      },
                      {
                        "indicator": "Our ability to learn from mistakes",
                        "description": "If the same bugs keep appearing again and again, then this is a symptom of our inability to incorporate learnings/lessons into improving how we develop - which could be indicative of other issues impeding our ability to embed quality into development - e.g. communication between dev & QA, dev & QA experience/craft mastery, unfamiliarity with the code base, etc.",
                        "metrics": [
                          {
                            "metric_name": "% of recurring bugs (that is, bugs that are raised in timebox n that are similar to bugs that were previously raised in past timeboxes)",
                            "value": 93.7,
                            "timeseries": [
                              90.7,
                              96.0,
                              91.5,
                              97.1,
                              100,
                              100,
                              93.5,
                              87.0,
                              87.9,
                              86.1,
                              92.2,
                              93.7
                            ],
                            "unit": "%",
                            "y_axis_label": "%"
                          }
                        ],
                        "data_source": "Jira",
                        "children": []
                      }
                    ]
                  },
                  {
                    "indicator": "Requirements coverage with test cases",
                    "description": "The comprehensiveness of our test cases and how they cover user stories is a leading indicator to quality - we rely on the requirements traceability matrix to assess how well requirements are covered",
                    "metrics": [
                      {
                        "metric_name": "story count coverage % by tests",
                        "value": 29.3,
                        "timeseries": [
                          15.9,
                          22.5,
                          27.3,
                          26.5,
                          26.3,
                          31.7,
                          22.5,
                          24.6,
                          20.2,
                          19.2,
                          22.7,
                          29.3
                        ],
                        "unit": "%",
                        "y_axis_label": "%"
                      },
                      {
                        "metric_name": "story point count coverage % by tests",
                        "value": 94.9,
                        "timeseries": [
                          81.1,
                          73.4,
                          69.8,
                          60.3,
                          76.7,
                          51.4,
                          89.3,
                          100,
                          100,
                          100,
                          100,
                          94.9
                        ],
                        "unit": "%",
                        "y_axis_label": "%"
                      },
                      {
                        "metric_name": "Execution rate",
                        "value": 9.6,
                        "timeseries": [
                          9.7,
                          10,
                          9.7,
                          10,
                          10,
                          10,
                          10,
                          7.1,
                          9.9,
                          9.3,
                          10,
                          9.6
                        ],
                        "unit": "score",
                        "y_axis_label": "Score (0\u201310)"
                      },
                      {
                        "metric_name": "# of test cases per work item per size",
                        "value": 61.0,
                        "timeseries": [
                          15.0,
                          26.0,
                          37.0,
                          43.0,
                          49.0,
                          48.0,
                          57.0,
                          59.0,
                          50.0,
                          52.0,
                          62.0,
                          61.0
                        ],
                        "unit": "count",
                        "y_axis_label": "Count"
                      },
                      {
                        "metric_name": "Variance of # of test cases per work item per size",
                        "value": 6.0,
                        "timeseries": [
                          2.0,
                          7.0,
                          0,
                          3.0,
                          9.0,
                          12.0,
                          19.0,
                          17.0,
                          3.0,
                          5.0,
                          0,
                          6.0
                        ],
                        "unit": "count",
                        "y_axis_label": "Count"
                      }
                    ],
                    "data_source": "Jira; Zephyr",
                    "children": []
                  },
                  {
                    "indicator": "Clarity of understanding regarding what value the work item is supposed to deliver",
                    "description": "If we release something that doesn't quite meet the needs of our customers, then it's inevitable that it would come back as production defect demand. We first need to determine how much of our prod defect demand is due to misunderstood requirements. This could be measured through metrics that show that things change - e.g. re-opened, flow-back, devs returning issue without adding stuff, comments, things changing after sprint starts like things being added etc. (esp. after first tests are executed), ",
                    "metrics": [
                      {
                        "metric_name": "Avg % of re-opened work items",
                        "value": 18.6,
                        "timeseries": [
                          31.4,
                          24.6,
                          21.9,
                          13.1,
                          16.5,
                          19.9,
                          0,
                          7.0,
                          7.3,
                          0.6,
                          9.0,
                          18.6
                        ],
                        "unit": "%",
                        "y_axis_label": "%"
                      },
                      {
                        "metric_name": "Avg # of times work item bounces between QA & Dev before moving forward",
                        "value": 6.0,
                        "timeseries": [
                          20.0,
                          25.0,
                          15.0,
                          6.0,
                          0,
                          12.0,
                          0,
                          6.0,
                          31.0,
                          6.0,
                          16.0,
                          6.0
                        ],
                        "unit": "count",
                        "y_axis_label": "Count"
                      },
                      {
                        "metric_name": "Avg # of times dev rejects bug",
                        "value": 53.0,
                        "timeseries": [
                          35.0,
                          29.0,
                          39.0,
                          48.0,
                          3.0,
                          1.0,
                          0,
                          0,
                          0,
                          6.0,
                          14.0,
                          53.0
                        ],
                        "unit": "count",
                        "y_axis_label": "Count"
                      },
                      {
                        "metric_name": "% of work items where 'comment sentiment' indicates lack of shared understanding of what work item should deliver",
                        "value": 89.9,
                        "timeseries": [
                          78.7,
                          76.6,
                          85.7,
                          55.6,
                          80.9,
                          79.9,
                          78.4,
                          74.1,
                          78.9,
                          100,
                          83.0,
                          89.9
                        ],
                        "unit": "%",
                        "y_axis_label": "%"
                      },
                      {
                        "metric_name": "% of work items where changes (added ACs, changes in description, estimate, splitting work item) happen after work item has moved to In Progress",
                        "value": 47.3,
                        "timeseries": [
                          2.1,
                          0,
                          0,
                          0.7,
                          6.1,
                          8.6,
                          16.8,
                          7.4,
                          21.7,
                          31.2,
                          39.3,
                          47.3
                        ],
                        "unit": "%",
                        "y_axis_label": "%"
                      },
                      {
                        "metric_name": "% of work items where changes (added ACs, changes in description, estimate, splitting work item) happen after work item has moved to In QA",
                        "value": 0,
                        "timeseries": [
                          23.1,
                          15.6,
                          19.8,
                          29.5,
                          0,
                          0,
                          8.7,
                          0.2,
                          19.0,
                          7.4,
                          0,
                          0
                        ],
                        "unit": "%",
                        "y_axis_label": "%"
                      },
                      {
                        "metric_name": "% of work items that go back to Dev after UAT or PO Review",
                        "value": 47.5,
                        "timeseries": [
                          75.2,
                          76.9,
                          67.1,
                          64.9,
                          70.1,
                          76.0,
                          74.4,
                          66.6,
                          60.7,
                          60.3,
                          51.0,
                          47.5
                        ],
                        "unit": "%",
                        "y_axis_label": "%"
                      },
                      {
                        "metric_name": "Team self-assessment score of how often there's a misunderstanding / disagreement between Devs, testers & Business (PO/BA) regarding what a piece of work is supposed to deliver",
                        "value": 3.6,
                        "timeseries": [
                          1.9,
                          0.9,
                          1.7,
                          1.4,
                          0,
                          0,
                          0.3,
                          0.6,
                          1.1,
                          2.7,
                          2.6,
                          3.6
                        ],
                        "unit": "score",
                        "y_axis_label": "Score (0\u201310)"
                      }
                    ],
                    "data_source": "Jira; Self-assessment",
                    "children": []
                  },
                  {
                    "indicator": "The presensce of dedicated Quality Assurance experts in the team",
                    "description": "In the situation where testers/QA specialists are not dedicated to the team and are instead shared between teams / or that there is some centralised 'testing team' that receives testing requests, we hypothesise that teams like that have a higher likelihood of escaped defects. The increased pressue on testing when less than 100% of a QA specialist's capacity is dedicated to the team could lead to less effort put into testing each individual piece of work, resulting in an increased likelihood of defects making their way to prod",
                    "metrics": [
                      {
                        "metric_name": "# of teams the team's QA is contributing to",
                        "value": 116.0,
                        "timeseries": [
                          59.0,
                          48.0,
                          41.0,
                          45.0,
                          47.0,
                          73.0,
                          78.0,
                          87.0,
                          82.0,
                          54.0,
                          69.0,
                          116.0
                        ],
                        "unit": "count",
                        "y_axis_label": "Count"
                      },
                      {
                        "metric_name": "% of the QA's capacity is dedicated to the team (from Jira)",
                        "value": 90.2,
                        "timeseries": [
                          44.0,
                          39.5,
                          41.0,
                          50.3,
                          90.2,
                          72.9,
                          96.4,
                          93.5,
                          100,
                          100,
                          100,
                          90.2
                        ],
                        "unit": "%",
                        "y_axis_label": "%"
                      },
                      {
                        "metric_name": "avg # of use cases per work item generated by part-time testers (compared to full-time testers, looking across multiple teams)",
                        "value": 8.1,
                        "timeseries": [
                          7.6,
                          7.2,
                          6.3,
                          7.2,
                          6.3,
                          5.7,
                          5.5,
                          4.7,
                          5.0,
                          6.7,
                          7.3,
                          8.1
                        ],
                        "unit": "score",
                        "y_axis_label": "Score (0\u201310)"
                      },
                      {
                        "metric_name": "% of bugs raised per work item by part-time testers",
                        "value": 59.7,
                        "timeseries": [
                          95.0,
                          94.6,
                          100,
                          100,
                          93.4,
                          53.6,
                          44.9,
                          29.8,
                          27.5,
                          52.0,
                          58.2,
                          59.7
                        ],
                        "unit": "%",
                        "y_axis_label": "%"
                      },
                      {
                        "metric_name": "% of fault feedback and bugs being rejected by Dev that were created by part-time testers",
                        "value": 17.5,
                        "timeseries": [
                          10.4,
                          13.2,
                          21.2,
                          18.3,
                          8.3,
                          6.5,
                          14.5,
                          0,
                          2.7,
                          16.3,
                          10.1,
                          17.5
                        ],
                        "unit": "%",
                        "y_axis_label": "%"
                      },
                      {
                        "metric_name": "Team self-assessment score of how much % is dedicated to each team or stream of work they support",
                        "value": 10,
                        "timeseries": [
                          4.4,
                          3.4,
                          3.5,
                          4.1,
                          4.8,
                          5.7,
                          5.1,
                          9.5,
                          9.3,
                          8.4,
                          9.1,
                          10
                        ],
                        "unit": "score",
                        "y_axis_label": "Score (0\u201310)"
                      }
                    ],
                    "data_source": "Jira; Zephyr; Self-assessment",
                    "children": []
                  }
                ]
              }
            ]
          }
        ]
      },
      {
        "indicator": "Volume of re-work",
        "description": "Rework refers to having to do the same thing more than once due to some error when we did it the first time. Re-work contributes to the amount of work in the system because it adds 'new' work - the work we have to re-do - on top of the other work we committed to deliver ",
        "metrics": [
          {
            "metric_name": "% of total rework volume to overall work volume -- total PR re-work volume + total bug/defect re-work volume + total re-work mandated by flow back from steps further down the value stream",
            "value": 41.7,
            "timeseries": [
              28.0,
              34.3,
              39.9,
              33.5,
              27.8,
              16.9,
              18.3,
              28.1,
              49.9,
              43.4,
              50.3,
              41.7
            ],
            "unit": "%",
            "y_axis_label": "%"
          }
        ],
        "data_source": "",
        "children": [
          {
            "indicator": "Re-work driven by QA",
            "description": "Just as quality affects WIP through the increase of escaped defects, quality can also affect WIP (perhaps more directly) by creating more failure demand. The poorer the quality, the more re-work is needed to fix defected features as they make their way downstream. We will explore the various leading indicators that affect the quality of the code that flows down to QA - the better the code quality, the lower the likelihood of needing re-work",
            "metrics": [
              {
                "metric_name": "% of total bug/defect re-work volume to total re-work volume",
                "value": 23.0,
                "timeseries": [
                  22.2,
                  17.8,
                  26.5,
                  19.9,
                  35.3,
                  9.0,
                  43.6,
                  42.8,
                  46.8,
                  38.5,
                  31.8,
                  23.0
                ],
                "unit": "%",
                "y_axis_label": "%"
              }
            ],
            "data_source": "",
            "children": [
              {
                "indicator": "Effectiveness of code reviews",
                "description": "The extent and effectiveness of code reviews are leading indicators to quality - our hypothesis is that subpar code reviews will result in higher defects making their way to QA. Code with higher defect rate = more bugs discovered = more rework",
                "metrics": [
                  {
                    "metric_name": "PR size (Avg. # of lines changed/files modified per PR)",
                    "value": 63.0,
                    "timeseries": [
                      9.0,
                      19.0,
                      21.0,
                      19.0,
                      11.0,
                      7.0,
                      0,
                      0,
                      6.0,
                      17.0,
                      24.0,
                      63.0
                    ],
                    "unit": "count",
                    "y_axis_label": "Count"
                  },
                  {
                    "metric_name": "Avg time spent on code reviews",
                    "value": 56.0,
                    "timeseries": [
                      67.0,
                      71.0,
                      70.0,
                      62.0,
                      70.0,
                      64.0,
                      63.0,
                      57.0,
                      59.0,
                      50.0,
                      55.0,
                      56.0
                    ],
                    "unit": "count",
                    "y_axis_label": "Count"
                  },
                  {
                    "metric_name": "Review depth (# of comments per PR)",
                    "value": 12.0,
                    "timeseries": [
                      62.0,
                      67.0,
                      73.0,
                      80.0,
                      69.0,
                      63.0,
                      56.0,
                      14.0,
                      22.0,
                      18.0,
                      22.0,
                      12.0
                    ],
                    "unit": "count",
                    "y_axis_label": "Count"
                  },
                  {
                    "metric_name": "Avg. time between PR creation and first review",
                    "value": 21.0,
                    "timeseries": [
                      45.0,
                      37.0,
                      41.0,
                      33.0,
                      23.0,
                      25.0,
                      23.0,
                      24.0,
                      28.0,
                      37.0,
                      27.0,
                      21.0
                    ],
                    "unit": "count",
                    "y_axis_label": "Count"
                  },
                  {
                    "metric_name": "Avg number of reviewers per PR",
                    "value": 31.0,
                    "timeseries": [
                      10.0,
                      6.0,
                      0,
                      5.0,
                      9.0,
                      0.0,
                      26.0,
                      0,
                      0,
                      25.0,
                      33.0,
                      31.0
                    ],
                    "unit": "count",
                    "y_axis_label": "Count"
                  },
                  {
                    "metric_name": "Time to merge (avg. time from PR creation to merge)",
                    "value": 53.0,
                    "timeseries": [
                      48.0,
                      58.0,
                      52.0,
                      58.0,
                      63.0,
                      57.0,
                      53.0,
                      66.0,
                      62.0,
                      68.0,
                      66.0,
                      53.0
                    ],
                    "unit": "count",
                    "y_axis_label": "Count"
                  },
                  {
                    "metric_name": "comment resolution time (Avg. ime to address review comments)",
                    "value": 60.0,
                    "timeseries": [
                      36.0,
                      39.0,
                      29.0,
                      39.0,
                      41.0,
                      64.0,
                      41.0,
                      45.0,
                      45.0,
                      47.0,
                      56.0,
                      60.0
                    ],
                    "unit": "count",
                    "y_axis_label": "Count"
                  }
                ],
                "data_source": "Bitbucket",
                "children": []
              },
              {
                "indicator": "Work item size",
                "description": "Larger work items are riskier, potentially leading to more quality issues (more bugs = more re-work). Also, the increased number of bugs increases the risk of some of them escaping QA, leading to more expensive re-work",
                "metrics": [
                  {
                    "metric_name": "% of work items that clear Dev in the same sprint they were started in",
                    "value": 38.2,
                    "timeseries": [
                      30.5,
                      35.5,
                      47.8,
                      34.8,
                      49.9,
                      50.8,
                      45.7,
                      74.8,
                      39.3,
                      37.9,
                      37.5,
                      38.2
                    ],
                    "unit": "%",
                    "y_axis_label": "%"
                  },
                  {
                    "metric_name": "Total Dev Touch Time",
                    "value": 21.8,
                    "timeseries": [
                      24.2,
                      25.9,
                      26.0,
                      24.8,
                      22.3,
                      30,
                      29.3,
                      27.1,
                      24.3,
                      23.4,
                      22.8,
                      21.8
                    ],
                    "unit": "days",
                    "y_axis_label": "Days"
                  },
                  {
                    "metric_name": "Standard deviation of how long in Dev work items of the same estimate take",
                    "value": 46.0,
                    "timeseries": [
                      58.0,
                      50.0,
                      50.0,
                      44.0,
                      53.0,
                      58.0,
                      47.0,
                      53.0,
                      45.0,
                      46.0,
                      38.0,
                      46.0
                    ],
                    "unit": "count",
                    "y_axis_label": "Count"
                  },
                  {
                    "metric_name": "what percentile is this team/ToT in, in terms of how long on average a piece of work takes",
                    "value": 64.8,
                    "timeseries": [
                      55.5,
                      58.8,
                      61.5,
                      68.5,
                      65.9,
                      71.5,
                      68.0,
                      67.7,
                      68.8,
                      76.4,
                      73.7,
                      64.8
                    ],
                    "unit": "%",
                    "y_axis_label": "%"
                  },
                  {
                    "metric_name": "avg number of sprints a piece of work is carried over before it clear development",
                    "value": 0,
                    "timeseries": [
                      2.0,
                      2.2,
                      1.8,
                      1.9,
                      1.6,
                      2.1,
                      1.5,
                      1.4,
                      2.4,
                      2.3,
                      2.2,
                      0
                    ],
                    "unit": "sprints",
                    "y_axis_label": "Number of Sprints"
                  }
                ],
                "data_source": "Jira",
                "children": []
              },
              {
                "indicator": "Unit test coverage",
                "description": "Lower test coverage increases the risk of latent bugs that are either identified at QA (which causes QA-driven failure demand & subsequent re-work), or (worse) escape QA and make their way to prod",
                "metrics": [
                  {
                    "metric_name": "% of code covered by unit tests",
                    "value": 27.7,
                    "timeseries": [
                      34.8,
                      38.7,
                      46.4,
                      33.7,
                      41.3,
                      51.0,
                      47.8,
                      44.1,
                      50.8,
                      72.6,
                      53.1,
                      27.7
                    ],
                    "unit": "%",
                    "y_axis_label": "%"
                  },
                  {
                    "metric_name": "Test coverage change (change in test coverage % per PR)",
                    "value": 70.2,
                    "timeseries": [
                      73.0,
                      75.2,
                      78.7,
                      77.6,
                      100,
                      100,
                      66.9,
                      65.5,
                      58.5,
                      65.9,
                      62.3,
                      70.2
                    ],
                    "unit": "%",
                    "y_axis_label": "%"
                  }
                ],
                "data_source": "Bitbucket",
                "children": []
              },
              {
                "indicator": "Code complexity",
                "description": "we hypothesise that  complexity, measured through cyclomatic complexity (CC), is a good leading indicator to quality. The more complex the code, the higher the likelihood of breaking something while trying to integrate something new AND the likelihood of not fixing a problem as intended, leading to more bugs and more re-work",
                "metrics": [
                  {
                    "metric_name": "Code Complexity (1-5 low, 6-10 moderate, 11-20 high, 21+ very high)",
                    "value": 0.3,
                    "timeseries": [
                      0.8,
                      1.2,
                      1.7,
                      1.2,
                      0.9,
                      0.7,
                      0,
                      0,
                      0,
                      0.1,
                      0,
                      0.3
                    ],
                    "unit": "score",
                    "y_axis_label": "Score (0\u201310)"
                  },
                  {
                    "metric_name": "Maintainability Index (<10 difficult to maintain",
                    "value": 77.7,
                    "timeseries": [
                      89.1,
                      86.1,
                      100,
                      100,
                      100,
                      68.5,
                      78.2,
                      83.6,
                      75.6,
                      73.3,
                      76.8,
                      77.7
                    ],
                    "unit": "index",
                    "y_axis_label": "Index"
                  },
                  {
                    "metric_name": "between 10-20 moderately maintainable",
                    "value": 4.0,
                    "timeseries": [
                      5.6,
                      5.9,
                      5.2,
                      4.7,
                      5.6,
                      5.5,
                      5.5,
                      5.5,
                      5.4,
                      5.0,
                      4.4,
                      4.0
                    ],
                    "unit": "score",
                    "y_axis_label": "Score (0\u201310)"
                  },
                  {
                    "metric_name": ">20 highly maintainable)",
                    "value": 98.0,
                    "timeseries": [
                      91.0,
                      97.0,
                      94.0,
                      91.0,
                      83.0,
                      79.0,
                      72.0,
                      65.0,
                      59.0,
                      98.0,
                      91.0,
                      98.0
                    ],
                    "unit": "count",
                    "y_axis_label": "Count"
                  }
                ],
                "data_source": "Bitbucket",
                "children": []
              },
              {
                "indicator": "Dependencies",
                "description": "Looking both at code and architectural dependencies - the hypothesis is that dependencies not only slow down development and testing, but also that the more complex our dependencies are, the higher the likelihood of introducing bugs (higher risk of inadvertently breaking something). The higher the likelihood of introducing bugs, the higher the likelihood of generating re-work",
                "metrics": [
                  {
                    "metric_name": "Code dependency index",
                    "value": 1.5,
                    "timeseries": [
                      31.4,
                      22.8,
                      24.8,
                      20.1,
                      26.1,
                      1.0,
                      6.0,
                      5.6,
                      0.4,
                      0.2,
                      0,
                      1.5
                    ],
                    "unit": "index",
                    "y_axis_label": "Index"
                  },
                  {
                    "metric_name": "Architectural dependency index",
                    "value": 14.1,
                    "timeseries": [
                      66.5,
                      71.1,
                      74.1,
                      74.2,
                      77.8,
                      73.6,
                      65.9,
                      58.2,
                      48.4,
                      42.5,
                      50.7,
                      14.1
                    ],
                    "unit": "index",
                    "y_axis_label": "Index"
                  }
                ],
                "data_source": "Bitbucket",
                "children": []
              },
              {
                "indicator": "Our ability to learn from mistakes",
                "description": "If the same bugs keep appearing again and again, then this is a symptom of our inability to incorporate learnings/lessons into improving how we develop - which could be indicative of other issues impeding our ability to embed quality into development - e.g. communication between dev & QA, dev & QA experience/craft mastery, unfamiliarity with the code base, etc.",
                "metrics": [
                  {
                    "metric_name": "% of recurring bugs (that is, bugs that are raised in timebox n that are similar to bugs that were previously raised in past timeboxes)",
                    "value": 67.0,
                    "timeseries": [
                      69.7,
                      66.0,
                      37.7,
                      53.1,
                      54.2,
                      72.2,
                      86.4,
                      100,
                      86.1,
                      80.0,
                      72.7,
                      67.0
                    ],
                    "unit": "%",
                    "y_axis_label": "%"
                  }
                ],
                "data_source": "Jira",
                "children": []
              },
              {
                "indicator": "Clarity of understanding regarding what value the work item is supposed to deliver",
                "description": "If there's no shared understanding regarding what the work item is supposed to deliver, then it's inevitable that bugs will be raised, potentially leading to re-work",
                "metrics": [
                  {
                    "metric_name": "Avg # of times work item bounces between QA & Dev before moving forward",
                    "value": 114.0,
                    "timeseries": [
                      81.0,
                      89.0,
                      96.0,
                      88.0,
                      83.0,
                      92.0,
                      120,
                      105.0,
                      116.0,
                      108.0,
                      118.0,
                      114.0
                    ],
                    "unit": "count",
                    "y_axis_label": "Count"
                  },
                  {
                    "metric_name": "Avg # of times dev rejects bug",
                    "value": 14.0,
                    "timeseries": [
                      23.0,
                      32.0,
                      27.0,
                      30.0,
                      33.0,
                      28.0,
                      40.0,
                      34.0,
                      49.0,
                      51.0,
                      48.0,
                      14.0
                    ],
                    "unit": "count",
                    "y_axis_label": "Count"
                  },
                  {
                    "metric_name": "% of work items where 'comment sentiment' indicates lack of shared understanding of what work item should deliver",
                    "value": 3.1,
                    "timeseries": [
                      32.2,
                      27.4,
                      24.6,
                      19.0,
                      22.7,
                      15.5,
                      11.0,
                      0,
                      0,
                      0,
                      7.0,
                      3.1
                    ],
                    "unit": "%",
                    "y_axis_label": "%"
                  },
                  {
                    "metric_name": "% of work items where changes (added ACs, changes in description, estimate, splitting work item) happen after work item has moved to In Progress",
                    "value": 89.8,
                    "timeseries": [
                      76.1,
                      82.8,
                      82.8,
                      82.9,
                      91.9,
                      78.4,
                      100,
                      98.8,
                      87.6,
                      88.9,
                      94.7,
                      89.8
                    ],
                    "unit": "%",
                    "y_axis_label": "%"
                  },
                  {
                    "metric_name": "% of work items where changes (added ACs, changes in description, estimate, splitting work item) happen after work item has moved to In QA",
                    "value": 97.2,
                    "timeseries": [
                      43.7,
                      49.3,
                      49.2,
                      51.5,
                      54.0,
                      62.3,
                      62.8,
                      74.3,
                      100,
                      93.1,
                      97.1,
                      97.2
                    ],
                    "unit": "%",
                    "y_axis_label": "%"
                  },
                  {
                    "metric_name": "% of work items that go back to Dev after UAT or PO Review",
                    "value": 81.9,
                    "timeseries": [
                      50.0,
                      54.1,
                      51.6,
                      61.4,
                      84.9,
                      90.5,
                      97.6,
                      100,
                      95.8,
                      81.0,
                      84.3,
                      81.9
                    ],
                    "unit": "%",
                    "y_axis_label": "%"
                  },
                  {
                    "metric_name": "Team self-assessment score of how often there's a misunderstanding / disagreement between Devs, testers & Business (PO/BA) regarding what a piece of work is supposed to deliver",
                    "value": 0,
                    "timeseries": [
                      1.4,
                      1.4,
                      1.0,
                      0.3,
                      2.3,
                      1.6,
                      0,
                      1.3,
                      0,
                      0.1,
                      0,
                      0
                    ],
                    "unit": "score",
                    "y_axis_label": "Score (0\u201310)"
                  }
                ],
                "data_source": "Jira; Self-assessment",
                "children": []
              },
              {
                "indicator": "Clarity and consistent application of entry and exit criteria",
                "description": "Having clear, shared & agreed-upon dev exit criteria / testing entry criteria will ensure consistent quality, reducing the number of bugs found (and therefore reducing re-work)",
                "metrics": [
                  {
                    "metric_name": "% of trivial / low level bugs",
                    "value": 48.8,
                    "timeseries": [
                      97.7,
                      100,
                      96.8,
                      100,
                      91.1,
                      94.5,
                      69.2,
                      29.6,
                      38.3,
                      39.2,
                      49.1,
                      48.8
                    ],
                    "unit": "%",
                    "y_axis_label": "%"
                  },
                  {
                    "metric_name": "% of recurring bugs",
                    "value": 66.7,
                    "timeseries": [
                      89.0,
                      91.6,
                      89.2,
                      86.0,
                      90.8,
                      97.4,
                      100,
                      100,
                      93.9,
                      88.7,
                      88.0,
                      66.7
                    ],
                    "unit": "%",
                    "y_axis_label": "%"
                  },
                  {
                    "metric_name": "variance in code-review metrics for different devs (PR size (Avg. # of lines changed/files modified per PR)",
                    "value": 47.0,
                    "timeseries": [
                      51.0,
                      55.0,
                      51.0,
                      43.0,
                      7.0,
                      80.0,
                      63.0,
                      68.0,
                      60.0,
                      62.0,
                      45.0,
                      47.0
                    ],
                    "unit": "count",
                    "y_axis_label": "Count"
                  },
                  {
                    "metric_name": "Avg time spent on code reviews",
                    "value": 46.0,
                    "timeseries": [
                      32.0,
                      33.0,
                      34.0,
                      36.0,
                      31.0,
                      35.0,
                      31.0,
                      20.0,
                      31.0,
                      39.0,
                      47.0,
                      46.0
                    ],
                    "unit": "count",
                    "y_axis_label": "Count"
                  },
                  {
                    "metric_name": "Review depth (# of comments per PR)",
                    "value": 37.0,
                    "timeseries": [
                      71.0,
                      78.0,
                      75.0,
                      87.0,
                      120,
                      63.0,
                      22.0,
                      28.0,
                      29.0,
                      36.0,
                      33.0,
                      37.0
                    ],
                    "unit": "count",
                    "y_axis_label": "Count"
                  },
                  {
                    "metric_name": "Avg. time between PR creation and first review",
                    "value": 50.0,
                    "timeseries": [
                      51.0,
                      51.0,
                      41.0,
                      49.0,
                      39.0,
                      44.0,
                      70.0,
                      72.0,
                      46.0,
                      49.0,
                      55.0,
                      50.0
                    ],
                    "unit": "count",
                    "y_axis_label": "Count"
                  },
                  {
                    "metric_name": "Avg number of reviewers per PR",
                    "value": 0,
                    "timeseries": [
                      0.0,
                      0,
                      2.0,
                      10.0,
                      8.0,
                      44.0,
                      54.0,
                      45.0,
                      37.0,
                      30.0,
                      24.0,
                      0
                    ],
                    "unit": "count",
                    "y_axis_label": "Count"
                  },
                  {
                    "metric_name": "Time to merge (avg. time from PR creation to merge)",
                    "value": 32.0,
                    "timeseries": [
                      25.0,
                      27.0,
                      20.0,
                      21.0,
                      17.0,
                      15.0,
                      16.0,
                      23.0,
                      26.0,
                      26.0,
                      32.0,
                      32.0
                    ],
                    "unit": "count",
                    "y_axis_label": "Count"
                  },
                  {
                    "metric_name": "comment resolution time (Avg. ime to address review comments))",
                    "value": 20.0,
                    "timeseries": [
                      16.0,
                      6.0,
                      9.0,
                      6.0,
                      16.0,
                      0,
                      2.0,
                      0,
                      24.0,
                      22.0,
                      29.0,
                      20.0
                    ],
                    "unit": "count",
                    "y_axis_label": "Count"
                  },
                  {
                    "metric_name": "Variance in unit test coverage",
                    "value": 54.9,
                    "timeseries": [
                      85.4,
                      84.0,
                      81.5,
                      81.4,
                      72.3,
                      83.5,
                      87.3,
                      77.5,
                      71.9,
                      64.8,
                      61.1,
                      54.9
                    ],
                    "unit": "%",
                    "y_axis_label": "%"
                  },
                  {
                    "metric_name": "Team (QA specialists) self-assessment score of the % of bugs raised that they believe should've been screened at Dev and never made it to testing",
                    "value": 10,
                    "timeseries": [
                      8.0,
                      7.8,
                      10,
                      8.4,
                      8.3,
                      8.4,
                      8.0,
                      8.7,
                      10,
                      10,
                      10,
                      10
                    ],
                    "unit": "score",
                    "y_axis_label": "Score (0\u201310)"
                  }
                ],
                "data_source": "Jira; Bitbucket; Self-assessment",
                "children": []
              }
            ]
          },
          {
            "indicator": "Re-work driven by further downstream steps",
            "description": "This refers to all re-work triggered from steps downstream of QA - that would include UAT/PO review, and further downstream steps (potentially outside the control of the team) incl. any integration/regression testing and all work needed to release a piece of work to production",
            "metrics": [
              {
                "metric_name": "% of total re-work triggered by steps further down of QA",
                "value": 11.3,
                "timeseries": [
                  26.1,
                  31.3,
                  23.7,
                  20.7,
                  16.4,
                  23.9,
                  16.0,
                  15.3,
                  13.4,
                  15.0,
                  20.1,
                  11.3
                ],
                "unit": "%",
                "y_axis_label": "%"
              },
              {
                "metric_name": "[includes (a) how long re-opened work items took from moment of re-open to moving to Done again",
                "value": 6.0,
                "timeseries": [
                  40.0,
                  37.0,
                  27.0,
                  24.0,
                  27.0,
                  50.0,
                  58.0,
                  44.0,
                  42.0,
                  42.0,
                  41.0,
                  6.0
                ],
                "unit": "count",
                "y_axis_label": "Count"
              },
              {
                "metric_name": "(b) issues added to a piece of work that was marked as done/resolved - in this case, calculate the total cycle time of these issues as total re-work time",
                "value": 85.0,
                "timeseries": [
                  66.0,
                  72.0,
                  74.0,
                  83.0,
                  88.0,
                  96.0,
                  100,
                  100,
                  100,
                  96.0,
                  77.0,
                  85.0
                ],
                "unit": "count",
                "y_axis_label": "Count"
              },
              {
                "metric_name": "(c) use labels e.g. integration-issue to denote re-work, calculate total cycle time of this work as pure re-work",
                "value": 42.0,
                "timeseries": [
                  6.0,
                  13.0,
                  23.0,
                  31.0,
                  23.0,
                  19.0,
                  22.0,
                  29.0,
                  33.0,
                  38.0,
                  37.0,
                  42.0
                ],
                "unit": "count",
                "y_axis_label": "Count"
              },
              {
                "metric_name": "(d) as with QA-driven re-work, if a piece of work returns to In Dev, calculate the total time it takes again until the after the step it stopped at as re-work",
                "value": 0.2,
                "timeseries": [
                  8.5,
                  7.7,
                  5.4,
                  4.6,
                  5.1,
                  6.2,
                  4.0,
                  5.5,
                  3.6,
                  0,
                  0,
                  0.2
                ],
                "unit": "days",
                "y_axis_label": "Days"
              },
              {
                "metric_name": "(e) use sentiment analysis to analyse features and work items to determine if they were due to re-work initiatied further downstream. If a work item was identified as such, calculate the time it takes as re-work time]",
                "value": 53.0,
                "timeseries": [
                  45.0,
                  53.0,
                  60.0,
                  64.0,
                  67.0,
                  75.0,
                  85.0,
                  49.0,
                  54.0,
                  56.0,
                  60.0,
                  53.0
                ],
                "unit": "count",
                "y_axis_label": "Count"
              }
            ],
            "data_source": "Jira",
            "children": [
              {
                "indicator": "Clarity about the entry criteria of downstream steps",
                "description": "When it's not clear what is needed to be present to enable effective downstream processing, the likelihood of additional work to be undertaken is higher (first re-work to get the work to a state ready for downstream work/review/processing, then re-work resulting from that review)",
                "metrics": [
                  {
                    "metric_name": "% of trivial / low level defects from downstream steps raised",
                    "value": 45.0,
                    "timeseries": [
                      62.2,
                      70.3,
                      63.6,
                      55.6,
                      39.0,
                      31.0,
                      27.9,
                      47.9,
                      41.7,
                      45.3,
                      40.3,
                      45.0
                    ],
                    "unit": "%",
                    "y_axis_label": "%"
                  },
                  {
                    "metric_name": "% of recurring defects",
                    "value": 0,
                    "timeseries": [
                      20.1,
                      28.7,
                      19.1,
                      11.7,
                      15.4,
                      0,
                      2.5,
                      10.8,
                      4.1,
                      0,
                      0.4,
                      0
                    ],
                    "unit": "%",
                    "y_axis_label": "%"
                  },
                  {
                    "metric_name": "% of substantial defects raised",
                    "value": 80.4,
                    "timeseries": [
                      79.1,
                      70.4,
                      77.9,
                      87.2,
                      93.9,
                      90.9,
                      58.4,
                      54.6,
                      80.7,
                      54.7,
                      59.2,
                      80.4
                    ],
                    "unit": "%",
                    "y_axis_label": "%"
                  },
                  {
                    "metric_name": "Variance per work item size for how long it takes to get a work item to the downstream step",
                    "value": 0,
                    "timeseries": [
                      14.0,
                      14.0,
                      20.0,
                      25.0,
                      19.0,
                      13.0,
                      0,
                      6.0,
                      0,
                      0,
                      2.0,
                      0
                    ],
                    "unit": "count",
                    "y_axis_label": "Count"
                  },
                  {
                    "metric_name": "Team self-assessment score of how confident they are that they know what the downstream steps are looking for and have the tools/guidelines to embed it",
                    "value": 7.0,
                    "timeseries": [
                      2.0,
                      2.3,
                      2.1,
                      2.6,
                      3.9,
                      2.2,
                      3.0,
                      3.5,
                      3.7,
                      6.0,
                      6.7,
                      7.0
                    ],
                    "unit": "score",
                    "y_axis_label": "Score (0\u201310)"
                  }
                ],
                "data_source": "Self-assessment",
                "children": []
              },
              {
                "indicator": "Promptness in engaging downstream steps early in our development efforts",
                "description": "Engaging downstream steps early lowers the likelihood of costly discovery of defects later downstream - shift left",
                "metrics": [
                  {
                    "metric_name": "the existence of NFRs incl. security requirements in the Acceptance Criteria of the work item or as part of its description/documentation",
                    "value": 7.0,
                    "timeseries": [
                      1.0,
                      0,
                      0,
                      1.0,
                      13.0,
                      14.0,
                      5.0,
                      0.0,
                      0,
                      11.0,
                      3.0,
                      7.0
                    ],
                    "unit": "count",
                    "y_axis_label": "Count"
                  },
                  {
                    "metric_name": "Team self-assessment score of how early we engage those stages in our planning",
                    "value": 9.2,
                    "timeseries": [
                      8.4,
                      8.7,
                      8.6,
                      9.4,
                      9.3,
                      10,
                      9.2,
                      8.4,
                      8.3,
                      9.3,
                      9.1,
                      9.2
                    ],
                    "unit": "score",
                    "y_axis_label": "Score (0\u201310)"
                  }
                ],
                "data_source": "Self-assessment",
                "children": []
              },
              {
                "indicator": "Continuation of engagement with downstream steps throughout early planning & upstream steps",
                "description": "To reduce the likelihood of downstream steps discovering major defects in the work, the team should be proactive in engaging downstream (compliance) every time a major pivot takes place (not only early on) ",
                "metrics": [
                  {
                    "metric_name": "the % of work items where there's a mention of further downstream steps (in sub-tasks within work items",
                    "value": 75.0,
                    "timeseries": [
                      77.7,
                      79.4,
                      87.7,
                      82.7,
                      72.8,
                      72.3,
                      63.0,
                      65.2,
                      70.0,
                      64.5,
                      68.2,
                      75.0
                    ],
                    "unit": "%",
                    "y_axis_label": "%"
                  },
                  {
                    "metric_name": "in comments",
                    "value": 2.0,
                    "timeseries": [
                      43.0,
                      41.0,
                      34.0,
                      35.0,
                      27.0,
                      3.0,
                      0.0,
                      0,
                      2.0,
                      8.0,
                      5.0,
                      2.0
                    ],
                    "unit": "count",
                    "y_axis_label": "Count"
                  },
                  {
                    "metric_name": "in new work items added to capture feedback/re-work from downstream steps before the work item is moved downstream)",
                    "value": 67.0,
                    "timeseries": [
                      83.0,
                      83.0,
                      84.0,
                      83.0,
                      83.0,
                      70.0,
                      77.0,
                      71.0,
                      77.0,
                      68.0,
                      59.0,
                      67.0
                    ],
                    "unit": "count",
                    "y_axis_label": "Count"
                  },
                  {
                    "metric_name": "Team self-assessment score of how strongly they agree with the sentiment that these downstream (mostly compliance) steps are continuously engaged so that changes/pivots are assessed and feedback given quickly",
                    "value": 8.0,
                    "timeseries": [
                      9.5,
                      8.9,
                      6.4,
                      8.0,
                      8.4,
                      9.0,
                      9.0,
                      8.8,
                      8.2,
                      8.7,
                      8.4,
                      8.0
                    ],
                    "unit": "score",
                    "y_axis_label": "Score (0\u201310)"
                  }
                ],
                "data_source": "Jira; Self-assessment",
                "children": []
              },
              {
                "indicator": "Availability of downstream steps to be engaged in early and continuous planning",
                "description": "It's not just about 'our' willingness  to engage downstream steps - a key element here is also whether those downstream steps are available to be engaged promptly and how effective they are in communicating their entry criteria",
                "metrics": [
                  {
                    "metric_name": "Avg. duration from sending work item to downstream steps to getting feedback/response",
                    "value": 23.7,
                    "timeseries": [
                      18.8,
                      21.3,
                      24.0,
                      21.3,
                      22.4,
                      22.4,
                      22.2,
                      20.3,
                      24.9,
                      25.0,
                      24.0,
                      23.7
                    ],
                    "unit": "days",
                    "y_axis_label": "Days"
                  },
                  {
                    "metric_name": "Variance in how long it takes from sending work item to downstream step to getting feedback/response",
                    "value": 77.0,
                    "timeseries": [
                      74.0,
                      65.0,
                      74.0,
                      80.0,
                      75.0,
                      100,
                      100,
                      92.0,
                      100,
                      70.0,
                      70.0,
                      77.0
                    ],
                    "unit": "count",
                    "y_axis_label": "Count"
                  },
                  {
                    "metric_name": "Avg. overall duration of all the wait times at downstream step until work moves beyond it",
                    "value": 26.8,
                    "timeseries": [
                      25.3,
                      27.1,
                      26.3,
                      28.3,
                      27.2,
                      28.0,
                      23.3,
                      14.1,
                      24.5,
                      25.6,
                      26.0,
                      26.8
                    ],
                    "unit": "days",
                    "y_axis_label": "Days"
                  },
                  {
                    "metric_name": "Variance in Avg. overall duration of all the wait times at downstream step until work moves beyond it",
                    "value": 11.3,
                    "timeseries": [
                      15.7,
                      14.3,
                      15.2,
                      13.4,
                      12.7,
                      18.2,
                      16.2,
                      15.3,
                      17.6,
                      11.7,
                      8.9,
                      11.3
                    ],
                    "unit": "days",
                    "y_axis_label": "Days"
                  },
                  {
                    "metric_name": "Team self-assessment score of how easy it is to get downstream steps (e.g. various compliance teams) engaged and promptness of their responses",
                    "value": 6.8,
                    "timeseries": [
                      1.9,
                      1.4,
                      2.0,
                      2.2,
                      0.1,
                      6.8,
                      6.8,
                      7.7,
                      8.2,
                      10,
                      9.5,
                      6.8
                    ],
                    "unit": "score",
                    "y_axis_label": "Score (0\u201310)"
                  }
                ],
                "data_source": "Jira; Self-assessment",
                "children": []
              },
              {
                "indicator": "Clarity of feedback provided by downstream step re what needs to be fixed",
                "description": "Lack of key info as part of the feedback increases the likelihood of more back-and-forth between up and down-stream steps, increasing re-work",
                "metrics": [
                  {
                    "metric_name": "Variance of how long it takes to resolve re-work raised (counted from moment re-work item was put in Progress all the way to Done)",
                    "value": 66.0,
                    "timeseries": [
                      66.0,
                      73.0,
                      79.0,
                      70.0,
                      71.0,
                      80.0,
                      77.0,
                      75.0,
                      73.0,
                      66.0,
                      57.0,
                      66.0
                    ],
                    "unit": "count",
                    "y_axis_label": "Count"
                  },
                  {
                    "metric_name": "Avg # of times a downstream defect goes back and forth before resolved",
                    "value": 96.0,
                    "timeseries": [
                      41.0,
                      35.0,
                      32.0,
                      25.0,
                      22.0,
                      69.0,
                      24.0,
                      26.0,
                      67.0,
                      67.0,
                      95.0,
                      96.0
                    ],
                    "unit": "count",
                    "y_axis_label": "Count"
                  }
                ],
                "data_source": "Jira",
                "children": []
              },
              {
                "indicator": "Effectiveness of our triage process to determine what to fix now vs wait",
                "description": "Not every request we receive is critical enough be addressed immediately at the risk of disrupting the timebox. An effective triage process would ensure that we have systemic checks we perform to ascertain the criticality of the request we're receiving",
                "metrics": [
                  {
                    "metric_name": "Avg % of downstream defect demand requests that were added to the 'product' backlog vs. the 'sprint' backlog",
                    "value": 88.9,
                    "timeseries": [
                      95.8,
                      91.5,
                      88.1,
                      99.1,
                      84.2,
                      78.9,
                      69.5,
                      68.8,
                      75.8,
                      85.2,
                      83.3,
                      88.9
                    ],
                    "unit": "%",
                    "y_axis_label": "%"
                  },
                  {
                    "metric_name": "Team self-assessment score of how effective the team thinks their downstream defect triage process is (in ensuring they only work on the most critical stuff)",
                    "value": 6.4,
                    "timeseries": [
                      7.3,
                      7.1,
                      6.9,
                      7.8,
                      7.2,
                      7.2,
                      7.4,
                      8.1,
                      7.1,
                      6.3,
                      7.2,
                      6.4
                    ],
                    "unit": "score",
                    "y_axis_label": "Score (0\u201310)"
                  }
                ],
                "data_source": "Jira; Self-assessment",
                "children": []
              }
            ]
          }
        ]
      }
    ]
  }
]